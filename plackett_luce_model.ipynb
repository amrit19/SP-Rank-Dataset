{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51cd0954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_0c0cbbf2ac0d2d41277f784ec115a700 NOW.\n",
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_0c0cbbf2ac0d2d41277f784ec115a700 NOW.\n",
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_0c0cbbf2ac0d2d41277f784ec115a700 NOW.\n",
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_0c0cbbf2ac0d2d41277f784ec115a700 NOW.\n",
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_0c0cbbf2ac0d2d41277f784ec115a700 NOW.\n",
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_0c0cbbf2ac0d2d41277f784ec115a700 NOW.\n",
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_0c0cbbf2ac0d2d41277f784ec115a700 NOW.\n",
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_0c0cbbf2ac0d2d41277f784ec115a700 NOW.\n",
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_0c0cbbf2ac0d2d41277f784ec115a700 NOW.\n",
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_0c0cbbf2ac0d2d41277f784ec115a700 NOW.\n",
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_0c0cbbf2ac0d2d41277f784ec115a700 NOW.\n",
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_0c0cbbf2ac0d2d41277f784ec115a700 NOW.\n",
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_0c0cbbf2ac0d2d41277f784ec115a700 NOW.\n",
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_0c0cbbf2ac0d2d41277f784ec115a700 NOW.\n",
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_0c0cbbf2ac0d2d41277f784ec115a700 NOW.\n",
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_0c0cbbf2ac0d2d41277f784ec115a700 NOW.\n",
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_0c0cbbf2ac0d2d41277f784ec115a700 NOW.\n",
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_0c0cbbf2ac0d2d41277f784ec115a700 NOW.\n",
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_0c0cbbf2ac0d2d41277f784ec115a700 NOW.\n",
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_0c0cbbf2ac0d2d41277f784ec115a700 NOW.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options set: (1, 7, 13, 19)\n",
      "Expert Rankings: [1, 7, 13, 19]\n",
      "Non-Expert Rankings: [7, 13, 1, 19]\n",
      "\n",
      "Options set: (2, 8, 14, 20)\n",
      "Expert Rankings: [2, 8, 14, 20]\n",
      "Non-Expert Rankings: [8, 2, 14, 20]\n",
      "\n",
      "Options set: (3, 9, 15, 21)\n",
      "Expert Rankings: [3, 9, 15, 21]\n",
      "Non-Expert Rankings: [9, 3, 15, 21]\n",
      "\n",
      "Options set: (5, 11, 17, 23)\n",
      "Expert Rankings: [5, 11, 17, 23]\n",
      "Non-Expert Rankings: [11, 17, 5, 23]\n",
      "\n",
      "Options set: (7, 13, 19, 25)\n",
      "Expert Rankings: [7, 13, 19, 25]\n",
      "Non-Expert Rankings: [13, 19, 7, 25]\n",
      "\n",
      "Options set: (9, 15, 21, 27)\n",
      "Expert Rankings: [9, 15, 21, 27]\n",
      "Non-Expert Rankings: [21, 15, 9, 27]\n",
      "\n",
      "Options set: (11, 17, 23, 29)\n",
      "Expert Rankings: [11, 17, 23, 29]\n",
      "Non-Expert Rankings: [17, 11, 23, 29]\n",
      "\n",
      "Options set: (12, 18, 24, 30)\n",
      "Expert Rankings: [12, 18, 24, 30]\n",
      "Non-Expert Rankings: [18, 24, 12, 30]\n",
      "\n",
      "Options set: (13, 19, 25, 31)\n",
      "Expert Rankings: [13, 19, 25, 31]\n",
      "Non-Expert Rankings: [19, 25, 13, 31]\n",
      "\n",
      "Options set: (15, 21, 27, 33)\n",
      "Expert Rankings: [15, 21, 27, 33]\n",
      "Non-Expert Rankings: [21, 27, 15, 33]\n",
      "\n",
      "Options set: (17, 23, 29, 35)\n",
      "Expert Rankings: [17, 23, 29, 35]\n",
      "Non-Expert Rankings: [29, 17, 23, 35]\n",
      "\n",
      "Options set: (19, 25, 31, 37)\n",
      "Expert Rankings: [19, 25, 31, 37]\n",
      "Non-Expert Rankings: [25, 19, 31, 37]\n",
      "\n",
      "Options set: (21, 27, 33, 39)\n",
      "Expert Rankings: [21, 27, 33, 39]\n",
      "Non-Expert Rankings: [27, 33, 21, 39]\n",
      "\n",
      "Options set: (22, 28, 34, 40)\n",
      "Expert Rankings: [22, 28, 34, 40]\n",
      "Non-Expert Rankings: [28, 34, 22, 40]\n",
      "\n",
      "Options set: (23, 29, 35, 41)\n",
      "Expert Rankings: [23, 29, 35, 41]\n",
      "Non-Expert Rankings: [29, 35, 41, 23]\n",
      "\n",
      "Options set: (25, 31, 37, 43)\n",
      "Expert Rankings: [25, 31, 37, 43]\n",
      "Non-Expert Rankings: [31, 37, 25, 43]\n",
      "\n",
      "Options set: (27, 33, 39, 45)\n",
      "Expert Rankings: [27, 33, 39, 45]\n",
      "Non-Expert Rankings: [33, 39, 27, 45]\n",
      "\n",
      "Options set: (29, 35, 41, 47)\n",
      "Expert Rankings: [29, 35, 41, 47]\n",
      "Non-Expert Rankings: [41, 35, 29, 47]\n",
      "\n",
      "Options set: (31, 37, 43, 49)\n",
      "Expert Rankings: [31, 37, 43, 49]\n",
      "Non-Expert Rankings: [37, 43, 31, 49]\n",
      "\n",
      "Options set: (32, 38, 44, 50)\n",
      "Expert Rankings: [32, 38, 44, 50]\n",
      "Non-Expert Rankings: [38, 44, 32, 50]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import pystan\n",
    "import arviz as az\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('Elicitation Formats/rank-rank/rank-rank_1_4.csv')\n",
    "\n",
    "# Convert string representations of lists into actual Python lists\n",
    "df['options'] = df['options'].apply(ast.literal_eval)\n",
    "df['votes'] = df['votes'].apply(ast.literal_eval)\n",
    "df['predictions'] = df['predictions'].apply(ast.literal_eval)\n",
    "\n",
    "# Convert lists in 'options' to tuples for hashability\n",
    "df['options'] = df['options'].apply(tuple)\n",
    "\n",
    "# Function to preprocess and convert rankings to indices\n",
    "def convert_rankings_to_indices(rankings, options):\n",
    "    option_to_index = {option: i + 1 for i, option in enumerate(sorted(options))}\n",
    "    return [option_to_index[item] for item in rankings]\n",
    "\n",
    "# Group by options\n",
    "grouped = df.groupby('options')\n",
    "\n",
    "# Dictionary to store models and fits for each option set\n",
    "models = {}\n",
    "fits = {}\n",
    "rankings = {}\n",
    "\n",
    "# Stan model code\n",
    "stan_model_code = \"\"\"\n",
    "functions {\n",
    "    real plackett_luce_lpmf(int[] ranking, vector strengths, int[] ground_truth) {\n",
    "        real log_p = 0;\n",
    "        for (k in 1:size(ranking)) {\n",
    "            int item = ground_truth[ranking[k]];\n",
    "            real sum_strengths = 0;\n",
    "            for (m in k:size(ranking)) {\n",
    "                int idx = ground_truth[ranking[m]];\n",
    "                sum_strengths += strengths[idx];\n",
    "            }\n",
    "            log_p += log(strengths[item] / sum_strengths);\n",
    "        }\n",
    "        return log_p;\n",
    "    }\n",
    "}\n",
    "\n",
    "data {\n",
    "    int<lower=1> J;  // Number of questions\n",
    "    int<lower=1> K;  // Number of options per question\n",
    "    int<lower=1> N;  // Total number of unique options\n",
    "    int<lower=1, upper=N> vote_indices[J, K]; // Indices of votes\n",
    "    int<lower=1, upper=N> prediction_indices[J, K]; // Indices of predictions\n",
    "    int<lower=1, upper=N> ground_truth_indices[J, K]; // Indices of ground truth\n",
    "}\n",
    "\n",
    "parameters {\n",
    "    simplex[N] strengths_expert_vote;     // Normalized strength parameters for expert votes\n",
    "    simplex[N] strengths_nonexpert_vote;  // Normalized strength parameters for non-expert votes\n",
    "    simplex[N] strengths_expert_prediction;     // Normalized strength parameters for expert predictions\n",
    "    simplex[N] strengths_nonexpert_prediction;  // Normalized strength parameters for non-expert predictions\n",
    "    simplex[2] pi;                   // Mixing proportions for expert and non-expert\n",
    "}\n",
    "\n",
    "model {\n",
    "    // Priors for votes and predictions\n",
    "    strengths_expert_vote ~ dirichlet(rep_vector(3, N));\n",
    "    strengths_nonexpert_vote ~ dirichlet(rep_vector(1, N));\n",
    "    strengths_expert_prediction ~ dirichlet(rep_vector(1, N));\n",
    "    strengths_nonexpert_prediction ~ dirichlet(rep_vector(1, N));\n",
    "\n",
    "    pi ~ dirichlet([1, 3]');\n",
    "\n",
    "    // Prior constraints for votes\n",
    "    for (i in 1:(N-1)) {\n",
    "        target += normal_lpdf(strengths_expert_vote[i+1] | strengths_expert_vote[i] - 0.05, 0.025);\n",
    "        target += normal_lpdf(sum(strengths_expert_vote[1:i]) | sum(strengths_nonexpert_vote[1:i]) + (N-i-1)*0.05, 0.025);\n",
    "\n",
    "        // Prior constraints for predictions (same logic as votes)\n",
    "        target += normal_lpdf(strengths_expert_prediction[i+1] | strengths_expert_prediction[i] - 0.05, 0.025);\n",
    "        target += normal_lpdf(sum(strengths_expert_prediction[1:i]) | sum(strengths_nonexpert_prediction[1:i]) + (N-i-1)*0.05, 0.025);\n",
    "    }\n",
    "\n",
    "    // Likelihood for votes\n",
    "    for (j in 1:J) {\n",
    "        vector[2] log_probs_votes;\n",
    "        vector[2] log_probs_predictions;\n",
    "\n",
    "        log_probs_votes[1] = log(pi[1]) + plackett_luce_lpmf(vote_indices[j] | strengths_expert_vote, ground_truth_indices[j]);\n",
    "        log_probs_votes[2] = log(pi[2]) + plackett_luce_lpmf(vote_indices[j] | strengths_nonexpert_vote, ground_truth_indices[j]);\n",
    "\n",
    "        log_probs_predictions[1] = log(pi[1]) + plackett_luce_lpmf(prediction_indices[j] | strengths_expert_prediction, ground_truth_indices[j]);\n",
    "        log_probs_predictions[2] = log(pi[2]) + plackett_luce_lpmf(prediction_indices[j] | strengths_nonexpert_prediction, ground_truth_indices[j]);\n",
    "\n",
    "        target += log_sum_exp(log_probs_votes);\n",
    "        target += log_sum_exp(log_probs_predictions);\n",
    "    }\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "    vector[J] log_lik_votes;\n",
    "    vector[J] log_lik_predictions;\n",
    "    for (j in 1:J) {\n",
    "        vector[2] log_probs_votes;\n",
    "        vector[2] log_probs_predictions;\n",
    "\n",
    "        log_probs_votes[1] = log(pi[1]) + plackett_luce_lpmf(vote_indices[j] | strengths_expert_vote, ground_truth_indices[j]);\n",
    "        log_probs_votes[2] = log(pi[2]) + plackett_luce_lpmf(vote_indices[j] | strengths_nonexpert_vote, ground_truth_indices[j]);\n",
    "\n",
    "        log_probs_predictions[1] = log(pi[1]) + plackett_luce_lpmf(prediction_indices[j] | strengths_expert_prediction, ground_truth_indices[j]);\n",
    "        log_probs_predictions[2] = log(pi[2]) + plackett_luce_lpmf(prediction_indices[j] | strengths_nonexpert_prediction, ground_truth_indices[j]);\n",
    "\n",
    "        log_lik_votes[j] = log_sum_exp(log_probs_votes);\n",
    "        log_lik_predictions[j] = log_sum_exp(log_probs_predictions);\n",
    "    }\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Loop through each group\n",
    "for options, group in grouped:\n",
    "    # Apply the function to convert rankings to indices\n",
    "    group['vote_indices'] = group['votes'].apply(lambda x: convert_rankings_to_indices(x, options))\n",
    "    group['prediction_indices'] = group['predictions'].apply(lambda x: convert_rankings_to_indices(x, options))\n",
    "    group['ground_truth_indices'] = group['options'].apply(lambda x: convert_rankings_to_indices(x, options))\n",
    "\n",
    "    # Prepare data for the Stan model\n",
    "    stan_data = {\n",
    "        'J': len(group),\n",
    "        'K': len(options),\n",
    "        'N': len(options),\n",
    "        'vote_indices': np.array(group['vote_indices'].tolist()),\n",
    "        'prediction_indices': np.array(group['prediction_indices'].tolist()),\n",
    "        'ground_truth_indices': np.array(group['ground_truth_indices'].tolist())\n",
    "    }\n",
    "\n",
    "    # Compile and fit the model for each group\n",
    "    sm = pystan.StanModel(model_code=stan_model_code)\n",
    "    fit = sm.sampling(data=stan_data, iter=2000, chains=4)\n",
    "    fits[str(options)] = fit\n",
    "\n",
    "    # Extract the median of the posterior distributions for strengths\n",
    "    expert_strengths = np.median(fit['strengths_expert_vote'], axis=0)\n",
    "    nonexpert_strengths = np.median(fit['strengths_nonexpert_vote'], axis=0)\n",
    "\n",
    "    # Generate rankings based on strengths\n",
    "    expert_ranking = sorted(zip(options, expert_strengths), key=lambda x: x[1], reverse=True)\n",
    "    nonexpert_ranking = sorted(zip(options, nonexpert_strengths), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Store rankings\n",
    "    rankings[str(options)] = {\n",
    "        'expert_ranking': [x[0] for x in expert_ranking],\n",
    "        'nonexpert_ranking': [x[0] for x in nonexpert_ranking]\n",
    "    }\n",
    "\n",
    "# Print rankings for each group\n",
    "for options, ranks in rankings.items():\n",
    "    print(f\"Options set: {options}\")\n",
    "    print(f\"Expert Rankings: {ranks['expert_ranking']}\")\n",
    "    print(f\"Non-Expert Rankings: {ranks['nonexpert_ranking']}\")\n",
    "    print()\n",
    "\n",
    "# Optionally, convert the fit object to arviz InferenceData\n",
    "idata = az.from_pystan(posterior=fit)\n",
    "models[str(options)] = idata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cc9fe27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Generating data for option set: (1, 7, 13, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating Votes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:38<00:00, 10.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Generating data for option set: (2, 8, 14, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating Votes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:34<00:00, 10.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Generating data for option set: (3, 9, 15, 21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating Votes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:35<00:00, 10.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Generating data for option set: (5, 11, 17, 23)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating Votes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:33<00:00, 10.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Generating data for option set: (7, 13, 19, 25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating Votes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:40<00:00,  9.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Generating data for option set: (9, 15, 21, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating Votes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:39<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Generating data for option set: (11, 17, 23, 29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating Votes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:38<00:00, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Generating data for option set: (12, 18, 24, 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating Votes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:42<00:00,  9.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Generating data for option set: (13, 19, 25, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating Votes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:35<00:00, 10.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Generating data for option set: (15, 21, 27, 33)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating Votes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:35<00:00, 10.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Generating data for option set: (17, 23, 29, 35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating Votes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:36<00:00, 10.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Generating data for option set: (19, 25, 31, 37)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating Votes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:34<00:00, 10.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Generating data for option set: (21, 27, 33, 39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating Votes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:35<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Generating data for option set: (22, 28, 34, 40)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating Votes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:38<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Generating data for option set: (23, 29, 35, 41)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating Votes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:35<00:00, 10.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Generating data for option set: (25, 31, 37, 43)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating Votes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:34<00:00, 10.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Generating data for option set: (27, 33, 39, 45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating Votes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:35<00:00, 10.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Generating data for option set: (29, 35, 41, 47)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating Votes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:35<00:00, 10.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Generating data for option set: (31, 37, 43, 49)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating Votes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:34<00:00, 10.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Generating data for option set: (32, 38, 44, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating Votes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:35<00:00, 10.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ All synthetic votes saved to combined_plackett_luce_synthetic_votes.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "from itertools import permutations\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "import ast  # to parse keys from fits\n",
    "\n",
    "# -----------------------------\n",
    "# Define Simulation Classes\n",
    "# -----------------------------\n",
    "\n",
    "class Vote:\n",
    "    def __init__(self, question_number: int, options: List[int], ranking: List[int], predicted_probs: dict, is_expert: bool):\n",
    "        self.question_number = question_number\n",
    "        self.options = options\n",
    "        self.ranking = ranking\n",
    "        self.predicted_probs = predicted_probs\n",
    "        self.is_expert = is_expert\n",
    "\n",
    "class Voter:\n",
    "    def __init__(self, is_expert: bool, central_strengths: List[float], variance_expert: float, variance_nonexpert: float):\n",
    "        self.is_expert = is_expert\n",
    "        self.strength_params = self.sample_strength_params(central_strengths, variance_expert, variance_nonexpert)\n",
    "\n",
    "    def sample_strength_params(self, central_strengths: List[float], variance_expert: float, variance_nonexpert: float):\n",
    "        if self.is_expert:\n",
    "            strengths = [abs(norm.rvs(loc=s, scale=variance_expert)) for s in central_strengths]\n",
    "        else:\n",
    "            strengths = [abs(norm.rvs(loc=s, scale=variance_nonexpert)) for s in central_strengths]\n",
    "        total_strength = sum(strengths)\n",
    "        return [s / total_strength for s in strengths]\n",
    "\n",
    "    def vote(self, question_number: int, options: List[int], ground_truth: List[int], all_worlds: List[Tuple[int]]):\n",
    "        signal = self.plackett_luce_vote(options)\n",
    "        conditional_probs = {\n",
    "            s_j: compute_conditional_prob(s_j, tuple(signal), all_worlds, ground_truth, self.is_expert, self.strength_params)\n",
    "            for s_j in all_worlds\n",
    "        }\n",
    "        predicted_probs = self.predict(signal, conditional_probs, all_worlds)\n",
    "        prediction = max(predicted_probs, key=predicted_probs.get)\n",
    "        return Vote(question_number, options, signal, prediction, self.is_expert)\n",
    "\n",
    "    def plackett_luce_vote(self, options):\n",
    "        remaining_options = options.copy()\n",
    "        ranking = []\n",
    "        while remaining_options:\n",
    "            current_strengths = [self.get_strength(opt) for opt in remaining_options]\n",
    "            probs = [strength / sum(current_strengths) for strength in current_strengths]\n",
    "            chosen = np.random.choice(remaining_options, p=probs)\n",
    "            ranking.append(chosen)\n",
    "            remaining_options.remove(chosen)\n",
    "        return ranking\n",
    "\n",
    "    def get_strength(self, option):\n",
    "        return self.strength_params[option % len(self.strength_params)]\n",
    "\n",
    "    def predict(self, signal, conditional_probs, all_worlds):\n",
    "        prediction_probs = {world: prob for world, prob in conditional_probs.items() if world != tuple(signal)}\n",
    "        total_prob = sum(prediction_probs.values())\n",
    "        return {world: prob / total_prob for world, prob in prediction_probs.items()}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Supporting Functions\n",
    "# -----------------------------\n",
    "\n",
    "computed_posteriors = {}\n",
    "\n",
    "def compute_posterior(signal, world, all_worlds, ground_truth, is_expert, strength_params):\n",
    "    key = (tuple(signal), tuple(world), tuple(ground_truth), is_expert)\n",
    "    if key in computed_posteriors:\n",
    "        return computed_posteriors[key]\n",
    "    prior = 1 / len(all_worlds)\n",
    "    likelihood = plackett_luce_probability(signal, world, strength_params)\n",
    "    total_signal_prob = sum(plackett_luce_probability(signal, w, strength_params) * prior for w in all_worlds)\n",
    "    posterior = likelihood * prior / total_signal_prob\n",
    "    computed_posteriors[key] = posterior\n",
    "    return posterior\n",
    "\n",
    "def compute_conditional_prob(s_j, s_k, all_worlds, ground_truth, is_expert, strength_params, num_samples=500):\n",
    "    weights = [compute_posterior(s_k, w, all_worlds, ground_truth, is_expert, strength_params) for w in all_worlds]\n",
    "    sampled_worlds = random.choices(all_worlds, weights=weights, k=num_samples)\n",
    "    return sum(plackett_luce_probability(s_j, w, strength_params) for w in sampled_worlds) / num_samples\n",
    "\n",
    "def plackett_luce_probability(signal, world, strength_params):\n",
    "    prob = 1.0\n",
    "    remaining_options = list(world)\n",
    "    for item in signal:\n",
    "        if item in remaining_options:\n",
    "            current_strengths = [strength_params[opt % len(strength_params)] for opt in remaining_options]\n",
    "            total_strength = sum(current_strengths)\n",
    "            idx = remaining_options.index(item)\n",
    "            prob *= current_strengths[idx] / total_strength\n",
    "            remaining_options.pop(idx)\n",
    "    return prob\n",
    "\n",
    "def simulate_voting(num_voters: int, subset: List[int], ground_truth: List[int], central_strengths: List[float], variance_expert: float, variance_nonexpert: float, prob_expert: float) -> List[Vote]:\n",
    "    votes = []\n",
    "    question_number = 1\n",
    "    all_worlds = list(permutations(subset))\n",
    "    num_experts = np.random.binomial(num_voters, prob_expert)\n",
    "    voters = [Voter(False, central_strengths, variance_expert, variance_nonexpert) for _ in range(num_voters)]\n",
    "    expert_indices = np.random.choice(num_voters, size=num_experts, replace=False)\n",
    "    for idx in expert_indices:\n",
    "        voters[idx].is_expert = True\n",
    "    for voter in tqdm(voters, desc=\"Simulating Votes\"):\n",
    "        vote = voter.vote(question_number, subset, ground_truth, all_worlds)\n",
    "        votes.append(vote)\n",
    "    return votes\n",
    "\n",
    "# -----------------------------\n",
    "# Main Generator from Fits\n",
    "# -----------------------------\n",
    "\n",
    "def generate_and_combine_synthetic_votes(fits_dict, num_voters, output_path):\n",
    "    all_votes = []\n",
    "    for key, fit in fits_dict.items():\n",
    "        print(f\"üì¶ Generating data for option set: {key}\")\n",
    "        try:\n",
    "            # Extract median strengths and normalize\n",
    "            central_strengths = np.median(fit['strengths_expert_vote'], axis=0).tolist()\n",
    "            central_strengths = [s / sum(central_strengths) for s in central_strengths]\n",
    "\n",
    "            # Empirical variance from posterior samples\n",
    "            variance_expert = np.mean(np.var(fit['strengths_expert_vote'], axis=0))\n",
    "            variance_nonexpert = np.mean(np.var(fit['strengths_nonexpert_vote'], axis=0))\n",
    "\n",
    "            # Expert mixing proportion from pi\n",
    "            prob_expert = np.median(fit['pi'][:, 0])\n",
    "\n",
    "            subset = list(ast.literal_eval(key))\n",
    "            ground_truth = sorted(subset)\n",
    "\n",
    "            # Simulate\n",
    "            votes = simulate_voting(\n",
    "                num_voters=num_voters,\n",
    "                subset=subset,\n",
    "                ground_truth=ground_truth,\n",
    "                central_strengths=central_strengths,\n",
    "                variance_expert=variance_expert,\n",
    "                variance_nonexpert=variance_nonexpert,\n",
    "                prob_expert=prob_expert\n",
    "            )\n",
    "\n",
    "            # Collect all\n",
    "            for vote in votes:\n",
    "                all_votes.append([\n",
    "                    vote.question_number,\n",
    "                    vote.options,\n",
    "                    vote.ranking,\n",
    "                    vote.predicted_probs,\n",
    "                    vote.is_expert,\n",
    "                    \"Synthetic\",\n",
    "                    \"PL\"\n",
    "                ])\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipping {key} due to error: {e}\")\n",
    "\n",
    "    # Combine into one DataFrame\n",
    "    df = pd.DataFrame(all_votes, columns=['question', 'options', 'votes', 'predictions', 'is_expert', 'domain', 'treatment'])\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n‚úÖ All synthetic votes saved to {output_path}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Execute Simulation\n",
    "# -----------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_path = \"combined_plackett_luce_synthetic_votes.csv\"\n",
    "    num_voters = 1000  # can adjust if needed\n",
    "\n",
    "    generate_and_combine_synthetic_votes(\n",
    "        fits_dict=fits,\n",
    "        num_voters=num_voters,\n",
    "        output_path=output_path\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce935fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_0c0cbbf2ac0d2d41277f784ec115a700 NOW.\n",
      "WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n",
      "WARNING:pystan:13 of 4000 iterations ended with a divergence (0.325 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n",
      "WARNING:pystan:Chain 2: E-BFMI = 0.00156\n",
      "WARNING:pystan:E-BFMI below 0.2 indicates you may need to reparameterize your model\n",
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_0c0cbbf2ac0d2d41277f784ec115a700 NOW.\n",
      "WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n",
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_0c0cbbf2ac0d2d41277f784ec115a700 NOW.\n",
      "WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n",
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_0c0cbbf2ac0d2d41277f784ec115a700 NOW.\n",
      "WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n",
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_0c0cbbf2ac0d2d41277f784ec115a700 NOW.\n",
      "WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n",
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_0c0cbbf2ac0d2d41277f784ec115a700 NOW.\n",
      "WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n",
      "WARNING:pystan:6 of 4000 iterations ended with a divergence (0.15 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n",
      "WARNING:pystan:990 of 4000 iterations saturated the maximum tree depth of 10 (24.8 %)\n",
      "WARNING:pystan:Run again with max_treedepth larger than 10 to avoid saturation\n",
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_0c0cbbf2ac0d2d41277f784ec115a700 NOW.\n",
      "WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n",
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_0c0cbbf2ac0d2d41277f784ec115a700 NOW.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import pystan\n",
    "import arviz as az\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('combined_plackett_luce_synthetic_votes.csv')\n",
    "\n",
    "\n",
    "# Convert string representations of lists into actual Python lists\n",
    "df['options'] = df['options'].apply(ast.literal_eval)\n",
    "df['votes'] = df['votes'].apply(ast.literal_eval)\n",
    "df['predictions'] = df['predictions'].apply(ast.literal_eval)\n",
    "\n",
    "# Convert lists in 'options' to tuples for hashability\n",
    "df['options'] = df['options'].apply(tuple)\n",
    "\n",
    "# Function to preprocess and convert rankings to indices\n",
    "def convert_rankings_to_indices(rankings, options):\n",
    "    option_to_index = {option: i + 1 for i, option in enumerate(sorted(options))}\n",
    "    return [option_to_index[item] for item in rankings]\n",
    "\n",
    "# Group by options\n",
    "grouped = df.groupby('options')\n",
    "\n",
    "# Dictionary to store models and fits for each option set\n",
    "models = {}\n",
    "synth_fits = {}\n",
    "rankings = {}\n",
    "\n",
    "# Stan model code\n",
    "stan_model_code = \"\"\"\n",
    "functions {\n",
    "    real plackett_luce_lpmf(int[] ranking, vector strengths, int[] ground_truth) {\n",
    "        real log_p = 0;\n",
    "        for (k in 1:size(ranking)) {\n",
    "            int item = ground_truth[ranking[k]];\n",
    "            real sum_strengths = 0;\n",
    "            for (m in k:size(ranking)) {\n",
    "                int idx = ground_truth[ranking[m]];\n",
    "                sum_strengths += strengths[idx];\n",
    "            }\n",
    "            log_p += log(strengths[item] / sum_strengths);\n",
    "        }\n",
    "        return log_p;\n",
    "    }\n",
    "}\n",
    "\n",
    "data {\n",
    "    int<lower=1> J;  // Number of questions\n",
    "    int<lower=1> K;  // Number of options per question\n",
    "    int<lower=1> N;  // Total number of unique options\n",
    "    int<lower=1, upper=N> vote_indices[J, K]; // Indices of votes\n",
    "    int<lower=1, upper=N> prediction_indices[J, K]; // Indices of predictions\n",
    "    int<lower=1, upper=N> ground_truth_indices[J, K]; // Indices of ground truth\n",
    "}\n",
    "\n",
    "parameters {\n",
    "    simplex[N] strengths_expert_vote;     // Normalized strength parameters for expert votes\n",
    "    simplex[N] strengths_nonexpert_vote;  // Normalized strength parameters for non-expert votes\n",
    "    simplex[N] strengths_expert_prediction;     // Normalized strength parameters for expert predictions\n",
    "    simplex[N] strengths_nonexpert_prediction;  // Normalized strength parameters for non-expert predictions\n",
    "    simplex[2] pi;                   // Mixing proportions for expert and non-expert\n",
    "}\n",
    "\n",
    "model {\n",
    "    // Priors for votes and predictions\n",
    "    strengths_expert_vote ~ dirichlet(rep_vector(3, N));\n",
    "    strengths_nonexpert_vote ~ dirichlet(rep_vector(1, N));\n",
    "    strengths_expert_prediction ~ dirichlet(rep_vector(1, N));\n",
    "    strengths_nonexpert_prediction ~ dirichlet(rep_vector(1, N));\n",
    "\n",
    "    pi ~ dirichlet([1, 3]');\n",
    "\n",
    "    // Prior constraints for votes\n",
    "    for (i in 1:(N-1)) {\n",
    "        target += normal_lpdf(strengths_expert_vote[i+1] | strengths_expert_vote[i] - 0.05, 0.025);\n",
    "        target += normal_lpdf(sum(strengths_expert_vote[1:i]) | sum(strengths_nonexpert_vote[1:i]) + (N-i-1)*0.05, 0.025);\n",
    "\n",
    "        // Prior constraints for predictions (same logic as votes)\n",
    "        target += normal_lpdf(strengths_expert_prediction[i+1] | strengths_expert_prediction[i] - 0.05, 0.025);\n",
    "        target += normal_lpdf(sum(strengths_expert_prediction[1:i]) | sum(strengths_nonexpert_prediction[1:i]) + (N-i-1)*0.05, 0.025);\n",
    "    }\n",
    "\n",
    "    // Likelihood for votes\n",
    "    for (j in 1:J) {\n",
    "        vector[2] log_probs_votes;\n",
    "        vector[2] log_probs_predictions;\n",
    "\n",
    "        log_probs_votes[1] = log(pi[1]) + plackett_luce_lpmf(vote_indices[j] | strengths_expert_vote, ground_truth_indices[j]);\n",
    "        log_probs_votes[2] = log(pi[2]) + plackett_luce_lpmf(vote_indices[j] | strengths_nonexpert_vote, ground_truth_indices[j]);\n",
    "\n",
    "        log_probs_predictions[1] = log(pi[1]) + plackett_luce_lpmf(prediction_indices[j] | strengths_expert_prediction, ground_truth_indices[j]);\n",
    "        log_probs_predictions[2] = log(pi[2]) + plackett_luce_lpmf(prediction_indices[j] | strengths_nonexpert_prediction, ground_truth_indices[j]);\n",
    "\n",
    "        target += log_sum_exp(log_probs_votes);\n",
    "        target += log_sum_exp(log_probs_predictions);\n",
    "    }\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "    vector[J] log_lik_votes;\n",
    "    vector[J] log_lik_predictions;\n",
    "    for (j in 1:J) {\n",
    "        vector[2] log_probs_votes;\n",
    "        vector[2] log_probs_predictions;\n",
    "\n",
    "        log_probs_votes[1] = log(pi[1]) + plackett_luce_lpmf(vote_indices[j] | strengths_expert_vote, ground_truth_indices[j]);\n",
    "        log_probs_votes[2] = log(pi[2]) + plackett_luce_lpmf(vote_indices[j] | strengths_nonexpert_vote, ground_truth_indices[j]);\n",
    "\n",
    "        log_probs_predictions[1] = log(pi[1]) + plackett_luce_lpmf(prediction_indices[j] | strengths_expert_prediction, ground_truth_indices[j]);\n",
    "        log_probs_predictions[2] = log(pi[2]) + plackett_luce_lpmf(prediction_indices[j] | strengths_nonexpert_prediction, ground_truth_indices[j]);\n",
    "\n",
    "        log_lik_votes[j] = log_sum_exp(log_probs_votes);\n",
    "        log_lik_predictions[j] = log_sum_exp(log_probs_predictions);\n",
    "    }\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Loop through each group\n",
    "for options, group in grouped:\n",
    "    # Apply the function to convert rankings to indices\n",
    "    group['vote_indices'] = group['votes'].apply(lambda x: convert_rankings_to_indices(x, options))\n",
    "    group['prediction_indices'] = group['predictions'].apply(lambda x: convert_rankings_to_indices(x, options))\n",
    "    group['ground_truth_indices'] = group['options'].apply(lambda x: convert_rankings_to_indices(x, options))\n",
    "\n",
    "    # Prepare data for the Stan model\n",
    "    stan_data = {\n",
    "        'J': len(group),\n",
    "        'K': len(options),\n",
    "        'N': len(options),\n",
    "        'vote_indices': np.array(group['vote_indices'].tolist()),\n",
    "        'prediction_indices': np.array(group['prediction_indices'].tolist()),\n",
    "        'ground_truth_indices': np.array(group['ground_truth_indices'].tolist())\n",
    "    }\n",
    "\n",
    "    # Compile and fit the model for each group\n",
    "    sm = pystan.StanModel(model_code=stan_model_code)\n",
    "    synth_fit = sm.sampling(data=stan_data, iter=2000, chains=4)\n",
    "    synth_fits [str(options)] = synth_fit\n",
    "\n",
    "    # Extract the median of the posterior distributions for strengths\n",
    "    expert_strengths = np.median(synth_fit['strengths_expert_vote'], axis=0)\n",
    "    nonexpert_strengths = np.median(synth_fit['strengths_nonexpert_vote'], axis=0)\n",
    "\n",
    "    # Generate rankings based on strengths\n",
    "    expert_ranking = sorted(zip(options, expert_strengths), key=lambda x: x[1], reverse=True)\n",
    "    nonexpert_ranking = sorted(zip(options, nonexpert_strengths), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Store rankings\n",
    "    rankings[str(options)] = {\n",
    "        'expert_ranking': [x[0] for x in expert_ranking],\n",
    "        'nonexpert_ranking': [x[0] for x in nonexpert_ranking]\n",
    "    }\n",
    "\n",
    "# Print rankings for each group\n",
    "for options, ranks in rankings.items():\n",
    "    print(f\"Options set: {options}\")\n",
    "    print(f\"Expert Rankings: {ranks['expert_ranking']}\")\n",
    "    print(f\"Non-Expert Rankings: {ranks['nonexpert_ranking']}\")\n",
    "    print()\n",
    "\n",
    "# Optionally, convert the fit object to arviz InferenceData\n",
    "idata = az.from_pystan(posterior=synth_fit)\n",
    "models[str(options)] = idata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da74f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mean_params(fit):\n",
    "    summary = fit.summary()\n",
    "    param_names = summary['summary_rownames']\n",
    "    means = summary['summary'][:, 0]  # First column is the mean\n",
    "    return dict(zip(param_names, means))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d317837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comparison_records = []\n",
    "\n",
    "# Loop over each options group that exists in both\n",
    "for key in fits.keys():\n",
    "    print(f\"\\nüìä Comparing parameter recovery for option set: {key}\")\n",
    "    try:\n",
    "        original_params = extract_mean_params(fits[key])\n",
    "        synthetic_params = extract_mean_params(synth_fits[key])\n",
    "\n",
    "        for param in ['pi[1]', 'pi[2]'] + \\\n",
    "                     [f'strengths_expert_vote[{i+1}]' for i in range(len(original_params) // 6)]:\n",
    "            rel_error = abs(original_params[param] - synthetic_params[param]) / original_params[param]\n",
    "            comparison_records.append({\n",
    "                'options': key,\n",
    "                'parameter': param,\n",
    "                'original': original_params[param],\n",
    "                'synthetic': synthetic_params[param],\n",
    "                'relative_error': rel_error\n",
    "            })\n",
    "            print(f\"{param}: Relative Error = {rel_error:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not compare for {key}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4719ad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = pd.DataFrame(comparison_records)\n",
    "df_compare.to_csv('parameter_recovery_comparison.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbb7b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize dictionaries to store relative errors for each parameter group\n",
    "relative_errors = {\n",
    "    'expert_vote': [],\n",
    "    'nonexpert_vote': [],\n",
    "    'expert_prediction': [],\n",
    "    'nonexpert_prediction': [],\n",
    "    'expert_proportion': []\n",
    "}\n",
    "\n",
    "# Iterate over each group of options\n",
    "for key in fits.keys():\n",
    "    try:\n",
    "        original_params = extract_mean_params(fits[key])\n",
    "        synthetic_params = extract_mean_params(synth_fits[key])\n",
    "\n",
    "        # Determine the number of options (N)\n",
    "        N = sum(1 for p in original_params if p.startswith('strengths_expert_vote'))\n",
    "\n",
    "        # Compute relative errors for each parameter group\n",
    "        for i in range(N):\n",
    "            # Expert vote strengths\n",
    "            param = f'strengths_expert_vote[{i+1}]'\n",
    "            rel_error = abs(original_params[param] - synthetic_params[param]) / original_params[param]\n",
    "            relative_errors['expert_vote'].append(rel_error)\n",
    "\n",
    "            # Non-expert vote strengths\n",
    "            param = f'strengths_nonexpert_vote[{i+1}]'\n",
    "            rel_error = abs(original_params[param] - synthetic_params[param]) / original_params[param]\n",
    "            relative_errors['nonexpert_vote'].append(rel_error)\n",
    "\n",
    "            # Expert prediction strengths\n",
    "            param = f'strengths_expert_prediction[{i+1}]'\n",
    "            rel_error = abs(original_params[param] - synthetic_params[param]) / original_params[param]\n",
    "            relative_errors['expert_prediction'].append(rel_error)\n",
    "\n",
    "            # Non-expert prediction strengths\n",
    "            param = f'strengths_nonexpert_prediction[{i+1}]'\n",
    "            rel_error = abs(original_params[param] - synthetic_params[param]) / original_params[param]\n",
    "            relative_errors['nonexpert_prediction'].append(rel_error)\n",
    "\n",
    "        # Expert proportion (pi[1])\n",
    "        rel_error_pi = abs(original_params['pi[1]'] - synthetic_params['pi[1]']) / original_params['pi[1]']\n",
    "        relative_errors['expert_proportion'].append(rel_error_pi)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not compare for {key}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ff4cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average relative errors\n",
    "average_errors = {param: np.mean(errors) for param, errors in relative_errors.items()}\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nüìä Aggregate Parameter Recovery Errors:\")\n",
    "for param_group, avg_error in average_errors.items():\n",
    "    print(f\"üîπ {param_group.replace('_', ' ').title()}: {avg_error:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5efd2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results to a DataFrame\n",
    "df_summary = pd.DataFrame([\n",
    "    {'Parameter Group': param_group.replace('_', ' ').title(), 'Average Relative Error': avg_error}\n",
    "    for param_group, avg_error in average_errors.items()\n",
    "])\n",
    "\n",
    "# Save to CSV\n",
    "df_summary.to_csv('aggregate_parameter_recovery_summary.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed4e9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import pystan\n",
    "import arviz as az\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('Elicitation Formats/rank-rank/rank-rank_1_4.csv')\n",
    "# Convert string representations of lists into actual Python lists\n",
    "df['options'] = df['options'].apply(ast.literal_eval)\n",
    "df['votes'] = df['votes'].apply(ast.literal_eval)\n",
    "df['predictions'] = df['predictions'].apply(ast.literal_eval)\n",
    "\n",
    "# Convert lists in 'options' to tuples for hashability\n",
    "df['options'] = df['options'].apply(tuple)\n",
    "\n",
    "# Function to preprocess and convert rankings to indices\n",
    "def convert_rankings_to_indices(rankings, options):\n",
    "    option_to_index = {option: i + 1 for i, option in enumerate(sorted(options))}\n",
    "    return [option_to_index[item] for item in rankings]\n",
    "\n",
    "# Group by options\n",
    "grouped = df.groupby('options')\n",
    "\n",
    "# Dictionary to store models and fits for each option set\n",
    "models = {}\n",
    "fits = {}\n",
    "rankings = {}\n",
    "\n",
    "# Stan model code (modified for both votes and predictions)\n",
    "stan_model_code = \"\"\"\n",
    "functions {\n",
    "    real plackett_luce_lpmf(int[] ranking, vector strengths, int[] ground_truth) {\n",
    "        real log_p = 0;\n",
    "        for (k in 1:size(ranking)) {\n",
    "            int item = ground_truth[ranking[k]];\n",
    "            real sum_strengths = 0;\n",
    "            for (m in k:size(ranking)) {\n",
    "                int idx = ground_truth[ranking[m]];\n",
    "                sum_strengths += strengths[idx];\n",
    "            }\n",
    "            log_p += log(strengths[item] / sum_strengths);\n",
    "        }\n",
    "        return log_p;\n",
    "    }\n",
    "}\n",
    "\n",
    "data {\n",
    "    int<lower=1> J;  // Number of questions\n",
    "    int<lower=1> K;  // Number of options per question\n",
    "    int<lower=1> N;  // Total number of unique options\n",
    "    int<lower=1, upper=N> vote_indices[J, K];  // Indices of votes\n",
    "    int<lower=1, upper=N> prediction_indices[J, K];  // Indices of predictions\n",
    "    int<lower=1, upper=N> ground_truth_indices[J, K];  // Indices of ground truth\n",
    "}\n",
    "\n",
    "parameters {\n",
    "    simplex[N] strengths_expert_vote;     // Normalized strength parameters for expert votes\n",
    "    simplex[N] strengths_intermediate_vote; // Normalized strength parameters for intermediate votes\n",
    "    simplex[N] strengths_nonexpert_vote;  // Normalized strength parameters for non-expert votes\n",
    "\n",
    "    simplex[N] strengths_expert_prediction;     // Normalized strength parameters for expert predictions\n",
    "    simplex[N] strengths_intermediate_prediction; // Normalized strength parameters for intermediate predictions\n",
    "    simplex[N] strengths_nonexpert_prediction;  // Normalized strength parameters for non-expert predictions\n",
    "\n",
    "    simplex[3] pi;                   // Mixing proportions for expert, intermediate, and non-expert\n",
    "}\n",
    "\n",
    "model {\n",
    "    // Priors for votes (different for expert, intermediate, and non-expert)\n",
    "    strengths_expert_vote ~ dirichlet(rep_vector(3, N));\n",
    "    strengths_intermediate_vote ~ dirichlet(rep_vector(2, N));\n",
    "    strengths_nonexpert_vote ~ dirichlet(rep_vector(1, N));\n",
    "\n",
    "    // Priors for predictions (different for expert, intermediate, and non-expert)\n",
    "    strengths_expert_prediction ~ dirichlet(rep_vector(1, N));\n",
    "    strengths_intermediate_prediction ~ dirichlet(rep_vector(1, N));\n",
    "    strengths_nonexpert_prediction ~ dirichlet(rep_vector(1, N));\n",
    "\n",
    "    pi ~ dirichlet([1, 2, 3]');\n",
    "\n",
    "    // Prior constraints for votes\n",
    "    for (i in 1:(N-1)) {\n",
    "        target += normal_lpdf(strengths_expert_vote[i+1] | strengths_expert_vote[i] - 0.05, 0.025);\n",
    "        target += normal_lpdf(sum(strengths_expert_vote[1:i]) | sum(strengths_intermediate_vote[1:i]) + (N-i-1)*0.05, 0.025);\n",
    "        target += normal_lpdf(sum(strengths_intermediate_vote[1:i]) | sum(strengths_nonexpert_vote[1:i]) + (N-i-1)*0.05, 0.025);\n",
    "\n",
    "        // Prior constraints for predictions (same logic as votes)\n",
    "        target += normal_lpdf(strengths_expert_prediction[i+1] | strengths_expert_prediction[i] - 0.05, 0.025);\n",
    "        target += normal_lpdf(sum(strengths_expert_prediction[1:i]) | sum(strengths_intermediate_prediction[1:i]) + (N-i-1)*0.05, 0.025);\n",
    "        target += normal_lpdf(sum(strengths_intermediate_prediction[1:i]) | sum(strengths_nonexpert_prediction[1:i]) + (N-i-1)*0.05, 0.025);\n",
    "    }\n",
    "\n",
    "    // Likelihood for both votes and predictions\n",
    "    for (j in 1:J) {\n",
    "        vector[3] log_probs_votes;\n",
    "        vector[3] log_probs_predictions;\n",
    "\n",
    "        // Likelihood for votes\n",
    "        log_probs_votes[1] = log(pi[1]) + plackett_luce_lpmf(vote_indices[j] | strengths_expert_vote, ground_truth_indices[j]);\n",
    "        log_probs_votes[2] = log(pi[2]) + plackett_luce_lpmf(vote_indices[j] | strengths_intermediate_vote, ground_truth_indices[j]);\n",
    "        log_probs_votes[3] = log(pi[3]) + plackett_luce_lpmf(vote_indices[j] | strengths_nonexpert_vote, ground_truth_indices[j]);\n",
    "\n",
    "        // Likelihood for predictions\n",
    "        log_probs_predictions[1] = log(pi[1]) + plackett_luce_lpmf(prediction_indices[j] | strengths_expert_prediction, ground_truth_indices[j]);\n",
    "        log_probs_predictions[2] = log(pi[2]) + plackett_luce_lpmf(prediction_indices[j] | strengths_intermediate_prediction, ground_truth_indices[j]);\n",
    "        log_probs_predictions[3] = log(pi[3]) + plackett_luce_lpmf(prediction_indices[j] | strengths_nonexpert_prediction, ground_truth_indices[j]);\n",
    "\n",
    "        // Adding the target likelihoods for both votes and predictions\n",
    "        target += log_sum_exp(log_probs_votes);\n",
    "        target += log_sum_exp(log_probs_predictions);\n",
    "    }\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "    vector[J] log_lik_votes;\n",
    "    vector[J] log_lik_predictions;\n",
    "    for (j in 1:J) {\n",
    "        vector[3] log_probs_votes;\n",
    "        vector[3] log_probs_predictions;\n",
    "\n",
    "        // Generate log likelihood for votes\n",
    "        log_probs_votes[1] = log(pi[1]) + plackett_luce_lpmf(vote_indices[j] | strengths_expert_vote, ground_truth_indices[j]);\n",
    "        log_probs_votes[2] = log(pi[2]) + plackett_luce_lpmf(vote_indices[j] | strengths_intermediate_vote, ground_truth_indices[j]);\n",
    "        log_probs_votes[3] = log(pi[3]) + plackett_luce_lpmf(vote_indices[j] | strengths_nonexpert_vote, ground_truth_indices[j]);\n",
    "\n",
    "        // Generate log likelihood for predictions\n",
    "        log_probs_predictions[1] = log(pi[1]) + plackett_luce_lpmf(prediction_indices[j] | strengths_expert_prediction, ground_truth_indices[j]);\n",
    "        log_probs_predictions[2] = log(pi[2]) + plackett_luce_lpmf(prediction_indices[j] | strengths_intermediate_prediction, ground_truth_indices[j]);\n",
    "        log_probs_predictions[3] = log(pi[3]) + plackett_luce_lpmf(prediction_indices[j] | strengths_nonexpert_prediction, ground_truth_indices[j]);\n",
    "\n",
    "        log_lik_votes[j] = log_sum_exp(log_probs_votes);\n",
    "        log_lik_predictions[j] = log_sum_exp(log_probs_predictions);\n",
    "    }\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for options, group in grouped:\n",
    "    # Apply the function to convert rankings to indices\n",
    "    group['vote_indices'] = group['votes'].apply(lambda x: convert_rankings_to_indices(x, options))\n",
    "    group['prediction_indices'] = group['predictions'].apply(lambda x: convert_rankings_to_indices(x, options))\n",
    "    group['ground_truth_indices'] = group['options'].apply(lambda x: convert_rankings_to_indices(x, options))\n",
    "\n",
    "    # Prepare data for the Stan model\n",
    "    stan_data = {\n",
    "        'J': len(group),\n",
    "        'K': len(options),\n",
    "        'N': len(options),\n",
    "        'vote_indices': np.array(group['vote_indices'].tolist()),\n",
    "        'prediction_indices': np.array(group['prediction_indices'].tolist()),\n",
    "        'ground_truth_indices': np.array(group['ground_truth_indices'].tolist())\n",
    "    }\n",
    "\n",
    "    # Compile and fit the model for each group\n",
    "    sm = pystan.StanModel(model_code=stan_model_code)\n",
    "    fit = sm.sampling(data=stan_data, iter=6000, warmup=2000, chains=4)\n",
    "    fits[str(options)] = fit\n",
    "\n",
    "    # Extract the median of the posterior distributions for vote strengths\n",
    "    expert_vote_strengths = np.median(fit['strengths_expert_vote'], axis=0)\n",
    "    intermediate_vote_strengths = np.median(fit['strengths_intermediate_vote'], axis=0)\n",
    "    nonexpert_vote_strengths = np.median(fit['strengths_nonexpert_vote'], axis=0)\n",
    "\n",
    "    # Extract the median of the posterior distributions for prediction strengths\n",
    "    expert_prediction_strengths = np.median(fit['strengths_expert_prediction'], axis=0)\n",
    "    intermediate_prediction_strengths = np.median(fit['strengths_intermediate_prediction'], axis=0)\n",
    "    nonexpert_prediction_strengths = np.median(fit['strengths_nonexpert_prediction'], axis=0)\n",
    "\n",
    "    # Generate rankings based on vote strengths\n",
    "    expert_vote_ranking = sorted(zip(options, expert_vote_strengths), key=lambda x: x[1], reverse=True)\n",
    "    intermediate_vote_ranking = sorted(zip(options, intermediate_vote_strengths), key=lambda x: x[1], reverse=True)\n",
    "    nonexpert_vote_ranking = sorted(zip(options, nonexpert_vote_strengths), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Generate rankings based on prediction strengths\n",
    "    expert_prediction_ranking = sorted(zip(options, expert_prediction_strengths), key=lambda x: x[1], reverse=True)\n",
    "    intermediate_prediction_ranking = sorted(zip(options, intermediate_prediction_strengths), key=lambda x: x[1], reverse=True)\n",
    "    nonexpert_prediction_ranking = sorted(zip(options, nonexpert_prediction_strengths), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Store rankings for votes and predictions\n",
    "    rankings[str(options)] = {\n",
    "        'expert_vote_ranking': [x[0] for x in expert_vote_ranking],\n",
    "        'intermediate_vote_ranking': [x[0] for x in intermediate_vote_ranking],\n",
    "        'nonexpert_vote_ranking': [x[0] for x in nonexpert_vote_ranking],\n",
    "        'expert_prediction_ranking': [x[0] for x in expert_prediction_ranking],\n",
    "        'intermediate_prediction_ranking': [x[0] for x in intermediate_prediction_ranking],\n",
    "        'nonexpert_prediction_ranking': [x[0] for x in nonexpert_prediction_ranking]\n",
    "    }\n",
    "\n",
    "# Print rankings for each group\n",
    "for options, ranks in rankings.items():\n",
    "    print(f\"Options set: {options}\")\n",
    "    print(f\"Expert Vote Rankings: {ranks['expert_vote_ranking']}\")\n",
    "    print(f\"Intermediate Vote Rankings: {ranks['intermediate_vote_ranking']}\")\n",
    "    print(f\"Non-Expert Vote Rankings: {ranks['nonexpert_vote_ranking']}\")\n",
    "    print(f\"Expert Prediction Rankings: {ranks['expert_prediction_ranking']}\")\n",
    "    print(f\"Intermediate Prediction Rankings: {ranks['intermediate_prediction_ranking']}\")\n",
    "    print(f\"Non-Expert Prediction Rankings: {ranks['nonexpert_prediction_ranking']}\")\n",
    "    print()\n",
    "\n",
    "# Optionally, convert the fit object to arviz InferenceData\n",
    "for options, fit in fits.items():\n",
    "    idata = az.from_pystan(posterior=fit)\n",
    "    models[str(options)] = idata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e070016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "from itertools import permutations\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "import ast  # to parse keys from fits\n",
    "\n",
    "# -----------------------------\n",
    "# Define Simulation Classes\n",
    "# -----------------------------\n",
    "\n",
    "class Vote:\n",
    "    def __init__(self, question_number: int, options: List[int], ranking: List[int], predicted_probs: dict, group: int):\n",
    "        self.question_number = question_number\n",
    "        self.options = options\n",
    "        self.ranking = ranking\n",
    "        self.predicted_probs = predicted_probs\n",
    "        self.group = group\n",
    "\n",
    "class Voter:\n",
    "    def __init__(self, group: int, strength_templates: List[List[float]], variances: List[float]):\n",
    "        self.group = group  # 0 = expert, 1 = intermediate, 2 = non-expert\n",
    "        self.strength_params = self.sample_strength_params(strength_templates[group], variances[group])\n",
    "\n",
    "    def sample_strength_params(self, central_strengths: List[float], variance: float):\n",
    "        strengths = [abs(norm.rvs(loc=s, scale=variance)) for s in central_strengths]\n",
    "        total_strength = sum(strengths)\n",
    "        return [s / total_strength for s in strengths]\n",
    "\n",
    "    def vote(self, question_number: int, options: List[int], ground_truth: List[int], all_worlds: List[Tuple[int]]):\n",
    "        signal = self.plackett_luce_vote(options)\n",
    "        conditional_probs = {\n",
    "            s_j: compute_conditional_prob(s_j, tuple(signal), all_worlds, ground_truth, self.group, self.strength_params)\n",
    "            for s_j in all_worlds\n",
    "        }\n",
    "        predicted_probs = self.predict(signal, conditional_probs, all_worlds)\n",
    "        prediction = max(predicted_probs, key=predicted_probs.get)\n",
    "        return Vote(question_number, options, signal, prediction, self.group)\n",
    "\n",
    "    def plackett_luce_vote(self, options):\n",
    "        remaining_options = options.copy()\n",
    "        ranking = []\n",
    "        while remaining_options:\n",
    "            current_strengths = [self.get_strength(opt) for opt in remaining_options]\n",
    "            probs = [strength / sum(current_strengths) for strength in current_strengths]\n",
    "            chosen = np.random.choice(remaining_options, p=probs)\n",
    "            ranking.append(chosen)\n",
    "            remaining_options.remove(chosen)\n",
    "        return ranking\n",
    "\n",
    "    def get_strength(self, option):\n",
    "        return self.strength_params[option % len(self.strength_params)]\n",
    "\n",
    "    def predict(self, signal, conditional_probs, all_worlds):\n",
    "        prediction_probs = {world: prob for world, prob in conditional_probs.items() if world != tuple(signal)}\n",
    "        total_prob = sum(prediction_probs.values())\n",
    "        return {world: prob / total_prob for world, prob in prediction_probs.items()}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Supporting Functions\n",
    "# -----------------------------\n",
    "\n",
    "computed_posteriors = {}\n",
    "\n",
    "def compute_posterior(signal, world, all_worlds, ground_truth, group, strength_params):\n",
    "    key = (tuple(signal), tuple(world), tuple(ground_truth), group)\n",
    "    if key in computed_posteriors:\n",
    "        return computed_posteriors[key]\n",
    "    prior = 1 / len(all_worlds)\n",
    "    likelihood = plackett_luce_probability(signal, world, strength_params)\n",
    "    total_signal_prob = sum(plackett_luce_probability(signal, w, strength_params) * prior for w in all_worlds)\n",
    "    posterior = likelihood * prior / total_signal_prob\n",
    "    computed_posteriors[key] = posterior\n",
    "    return posterior\n",
    "\n",
    "def compute_conditional_prob(s_j, s_k, all_worlds, ground_truth, group, strength_params, num_samples=500):\n",
    "    weights = [compute_posterior(s_k, w, all_worlds, ground_truth, group, strength_params) for w in all_worlds]\n",
    "    sampled_worlds = random.choices(all_worlds, weights=weights, k=num_samples)\n",
    "    return sum(plackett_luce_probability(s_j, w, strength_params) for w in sampled_worlds) / num_samples\n",
    "\n",
    "def plackett_luce_probability(signal, world, strength_params):\n",
    "    prob = 1.0\n",
    "    remaining_options = list(world)\n",
    "    for item in signal:\n",
    "        if item in remaining_options:\n",
    "            current_strengths = [strength_params[opt % len(strength_params)] for opt in remaining_options]\n",
    "            total_strength = sum(current_strengths)\n",
    "            idx = remaining_options.index(item)\n",
    "            prob *= current_strengths[idx] / total_strength\n",
    "            remaining_options.pop(idx)\n",
    "    return prob\n",
    "\n",
    "def simulate_voting(num_voters: int, subset: List[int], ground_truth: List[int],\n",
    "                    strength_templates: List[List[float]], variances: List[float], mixing_probs: List[float]) -> List[Vote]:\n",
    "\n",
    "    all_worlds = list(permutations(subset))\n",
    "    question_number = 1\n",
    "    votes = []\n",
    "\n",
    "    group_assignments = np.random.choice([0, 1, 2], size=num_voters, p=mixing_probs)\n",
    "    voters = [Voter(group=g, strength_templates=strength_templates, variances=variances) for g in group_assignments]\n",
    "\n",
    "    for voter in tqdm(voters, desc=\"Simulating Votes\"):\n",
    "        vote = voter.vote(question_number, subset, ground_truth, all_worlds)\n",
    "        votes.append(vote)\n",
    "\n",
    "    return votes\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main Generator from Fits\n",
    "# -----------------------------\n",
    "\n",
    "def generate_and_combine_synthetic_votes(fits_dict, num_voters, output_path):\n",
    "    all_votes = []\n",
    "    for key, fit in fits_dict.items():\n",
    "        print(f\"üì¶ Generating data for option set: {key}\")\n",
    "        try:\n",
    "            # Median strength vectors for 3 groups\n",
    "            expert_strengths = np.median(fit['strengths_expert_vote'], axis=0).tolist()\n",
    "            intermediate_strengths = np.median(fit['strengths_intermediate_vote'], axis=0).tolist()\n",
    "            nonexpert_strengths = np.median(fit['strengths_nonexpert_vote'], axis=0).tolist()\n",
    "\n",
    "            # Normalize\n",
    "            strength_templates = [\n",
    "                [s / sum(expert_strengths) for s in expert_strengths],\n",
    "                [s / sum(intermediate_strengths) for s in intermediate_strengths],\n",
    "                [s / sum(nonexpert_strengths) for s in nonexpert_strengths],\n",
    "            ]\n",
    "\n",
    "            # Variances\n",
    "            variances = [\n",
    "                np.mean(np.var(fit['strengths_expert_vote'], axis=0)),\n",
    "                np.mean(np.var(fit['strengths_intermediate_vote'], axis=0)),\n",
    "                np.mean(np.var(fit['strengths_nonexpert_vote'], axis=0)),\n",
    "            ]\n",
    "\n",
    "            # Mixing proportions\n",
    "            pi_median = np.median(fit['pi'], axis=0).tolist()\n",
    "            mixing_probs = [p / sum(pi_median) for p in pi_median]\n",
    "\n",
    "            subset = list(ast.literal_eval(key))\n",
    "            ground_truth = sorted(subset)\n",
    "\n",
    "            votes = simulate_voting(\n",
    "                num_voters=num_voters,\n",
    "                subset=subset,\n",
    "                ground_truth=ground_truth,\n",
    "                strength_templates=strength_templates,\n",
    "                variances=variances,\n",
    "                mixing_probs=mixing_probs\n",
    "            )\n",
    "\n",
    "            for vote in votes:\n",
    "                all_votes.append([\n",
    "                    vote.question_number,\n",
    "                    vote.options,\n",
    "                    vote.ranking,\n",
    "                    vote.predicted_probs,\n",
    "                    vote.group,\n",
    "                    \"Synthetic\",\n",
    "                    \"PL\"\n",
    "                ])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipping {key} due to error: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(all_votes, columns=['question', 'options', 'votes', 'predictions', 'group', 'domain', 'treatment'])\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n‚úÖ All synthetic votes saved to {output_path}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Execute Simulation\n",
    "# -----------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_path = \"group3_plackett_luce_synthetic_votes.csv\"\n",
    "    num_voters = 1000\n",
    "\n",
    "    generate_and_combine_synthetic_votes(\n",
    "        fits_dict=fits,\n",
    "        num_voters=num_voters,\n",
    "        output_path=output_path\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7e860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import pystan\n",
    "import arviz as az\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('group3_plackett_luce_synthetic_votes.csv')\n",
    "\n",
    "# Convert string representations of lists into actual Python lists\n",
    "df['options'] = df['options'].apply(ast.literal_eval)\n",
    "df['votes'] = df['votes'].apply(ast.literal_eval)\n",
    "df['predictions'] = df['predictions'].apply(ast.literal_eval)\n",
    "\n",
    "# Convert lists in 'options' to tuples for hashability\n",
    "df['options'] = df['options'].apply(tuple)\n",
    "\n",
    "# Function to preprocess and convert rankings to indices\n",
    "def convert_rankings_to_indices(rankings, options):\n",
    "    option_to_index = {option: i + 1 for i, option in enumerate(sorted(options))}\n",
    "    return [option_to_index[item] for item in rankings]\n",
    "\n",
    "# Group by options\n",
    "grouped = df.groupby('options')\n",
    "\n",
    "# Dictionary to store models and fits for each option set\n",
    "models = {}\n",
    "synth_fits = {}\n",
    "rankings = {}\n",
    "\n",
    "# Stan model code adapted for 3 groups\n",
    "stan_model_code = \"\"\"\n",
    "functions {\n",
    "    real plackett_luce_lpmf(int[] ranking, vector strengths, int[] ground_truth) {\n",
    "        real log_p = 0;\n",
    "        for (k in 1:size(ranking)) {\n",
    "            int item = ground_truth[ranking[k]];\n",
    "            real sum_strengths = 0;\n",
    "            for (m in k:size(ranking)) {\n",
    "                int idx = ground_truth[ranking[m]];\n",
    "                sum_strengths += strengths[idx];\n",
    "            }\n",
    "            log_p += log(strengths[item] / sum_strengths);\n",
    "        }\n",
    "        return log_p;\n",
    "    }\n",
    "}\n",
    "\n",
    "data {\n",
    "    int<lower=1> J;\n",
    "    int<lower=1> K;\n",
    "    int<lower=1> N;\n",
    "    int<lower=1, upper=N> vote_indices[J, K];\n",
    "    int<lower=1, upper=N> prediction_indices[J, K];\n",
    "    int<lower=1, upper=N> ground_truth_indices[J, K];\n",
    "}\n",
    "\n",
    "parameters {\n",
    "    simplex[N] strengths_expert_vote;\n",
    "    simplex[N] strengths_intermediate_vote;\n",
    "    simplex[N] strengths_nonexpert_vote;\n",
    "    simplex[N] strengths_expert_prediction;\n",
    "    simplex[N] strengths_intermediate_prediction;\n",
    "    simplex[N] strengths_nonexpert_prediction;\n",
    "    simplex[3] pi;\n",
    "}\n",
    "\n",
    "model {\n",
    "    strengths_expert_vote ~ dirichlet(rep_vector(3, N));\n",
    "    strengths_intermediate_vote ~ dirichlet(rep_vector(2, N));\n",
    "    strengths_nonexpert_vote ~ dirichlet(rep_vector(1, N));\n",
    "    strengths_expert_prediction ~ dirichlet(rep_vector(1, N));\n",
    "    strengths_intermediate_prediction ~ dirichlet(rep_vector(1, N));\n",
    "    strengths_nonexpert_prediction ~ dirichlet(rep_vector(1, N));\n",
    "\n",
    "    pi ~ dirichlet([1, 2, 3]');\n",
    "\n",
    "    for (i in 1:(N-1)) {\n",
    "        target += normal_lpdf(strengths_expert_vote[i+1] | strengths_expert_vote[i] - 0.05, 0.025);\n",
    "        target += normal_lpdf(sum(strengths_expert_vote[1:i]) | sum(strengths_intermediate_vote[1:i]) + (N-i-1)*0.05, 0.025);\n",
    "        target += normal_lpdf(sum(strengths_intermediate_vote[1:i]) | sum(strengths_nonexpert_vote[1:i]) + (N-i-1)*0.05, 0.025);\n",
    "\n",
    "        target += normal_lpdf(strengths_expert_prediction[i+1] | strengths_expert_prediction[i] - 0.05, 0.025);\n",
    "        target += normal_lpdf(sum(strengths_expert_prediction[1:i]) | sum(strengths_intermediate_prediction[1:i]) + (N-i-1)*0.05, 0.025);\n",
    "        target += normal_lpdf(sum(strengths_intermediate_prediction[1:i]) | sum(strengths_nonexpert_prediction[1:i]) + (N-i-1)*0.05, 0.025);\n",
    "    }\n",
    "\n",
    "    for (j in 1:J) {\n",
    "        vector[3] log_probs_votes;\n",
    "        vector[3] log_probs_predictions;\n",
    "\n",
    "        log_probs_votes[1] = log(pi[1]) + plackett_luce_lpmf(vote_indices[j] | strengths_expert_vote, ground_truth_indices[j]);\n",
    "        log_probs_votes[2] = log(pi[2]) + plackett_luce_lpmf(vote_indices[j] | strengths_intermediate_vote, ground_truth_indices[j]);\n",
    "        log_probs_votes[3] = log(pi[3]) + plackett_luce_lpmf(vote_indices[j] | strengths_nonexpert_vote, ground_truth_indices[j]);\n",
    "\n",
    "        log_probs_predictions[1] = log(pi[1]) + plackett_luce_lpmf(prediction_indices[j] | strengths_expert_prediction, ground_truth_indices[j]);\n",
    "        log_probs_predictions[2] = log(pi[2]) + plackett_luce_lpmf(prediction_indices[j] | strengths_intermediate_prediction, ground_truth_indices[j]);\n",
    "        log_probs_predictions[3] = log(pi[3]) + plackett_luce_lpmf(prediction_indices[j] | strengths_nonexpert_prediction, ground_truth_indices[j]);\n",
    "\n",
    "        target += log_sum_exp(log_probs_votes);\n",
    "        target += log_sum_exp(log_probs_predictions);\n",
    "    }\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "    vector[J] log_lik_votes;\n",
    "    vector[J] log_lik_predictions;\n",
    "    for (j in 1:J) {\n",
    "        vector[3] log_probs_votes;\n",
    "        vector[3] log_probs_predictions;\n",
    "\n",
    "        log_probs_votes[1] = log(pi[1]) + plackett_luce_lpmf(vote_indices[j] | strengths_expert_vote, ground_truth_indices[j]);\n",
    "        log_probs_votes[2] = log(pi[2]) + plackett_luce_lpmf(vote_indices[j] | strengths_intermediate_vote, ground_truth_indices[j]);\n",
    "        log_probs_votes[3] = log(pi[3]) + plackett_luce_lpmf(vote_indices[j] | strengths_nonexpert_vote, ground_truth_indices[j]);\n",
    "\n",
    "        log_probs_predictions[1] = log(pi[1]) + plackett_luce_lpmf(prediction_indices[j] | strengths_expert_prediction, ground_truth_indices[j]);\n",
    "        log_probs_predictions[2] = log(pi[2]) + plackett_luce_lpmf(prediction_indices[j] | strengths_intermediate_prediction, ground_truth_indices[j]);\n",
    "        log_probs_predictions[3] = log(pi[3]) + plackett_luce_lpmf(prediction_indices[j] | strengths_nonexpert_prediction, ground_truth_indices[j]);\n",
    "\n",
    "        log_lik_votes[j] = log_sum_exp(log_probs_votes);\n",
    "        log_lik_predictions[j] = log_sum_exp(log_probs_predictions);\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Loop through each group\n",
    "for options, group in grouped:\n",
    "    group['vote_indices'] = group['votes'].apply(lambda x: convert_rankings_to_indices(x, options))\n",
    "    group['prediction_indices'] = group['predictions'].apply(lambda x: convert_rankings_to_indices(x, options))\n",
    "    group['ground_truth_indices'] = group['options'].apply(lambda x: convert_rankings_to_indices(x, options))\n",
    "\n",
    "    stan_data = {\n",
    "        'J': len(group),\n",
    "        'K': len(options),\n",
    "        'N': len(options),\n",
    "        'vote_indices': np.array(group['vote_indices'].tolist()),\n",
    "        'prediction_indices': np.array(group['prediction_indices'].tolist()),\n",
    "        'ground_truth_indices': np.array(group['ground_truth_indices'].tolist())\n",
    "    }\n",
    "\n",
    "    sm = pystan.StanModel(model_code=stan_model_code)\n",
    "    synth_fit = sm.sampling(data=stan_data, iter=2000, chains=4)\n",
    "    synth_fits[str(options)] = synth_fit\n",
    "\n",
    "    expert_strengths = np.median(synth_fit['strengths_expert_vote'], axis=0)\n",
    "    intermediate_strengths = np.median(synth_fit['strengths_intermediate_vote'], axis=0)\n",
    "    nonexpert_strengths = np.median(synth_fit['strengths_nonexpert_vote'], axis=0)\n",
    "\n",
    "    expert_ranking = sorted(zip(options, expert_strengths), key=lambda x: x[1], reverse=True)\n",
    "    intermediate_ranking = sorted(zip(options, intermediate_strengths), key=lambda x: x[1], reverse=True)\n",
    "    nonexpert_ranking = sorted(zip(options, nonexpert_strengths), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    rankings[str(options)] = {\n",
    "        'expert_ranking': [x[0] for x in expert_ranking],\n",
    "        'intermediate_ranking': [x[0] for x in intermediate_ranking],\n",
    "        'nonexpert_ranking': [x[0] for x in nonexpert_ranking]\n",
    "    }\n",
    "\n",
    "    print(f\"Options set: {options}\")\n",
    "    print(f\"Expert Rankings: {rankings[str(options)]['expert_ranking']}\")\n",
    "    print(f\"Intermediate Rankings: {rankings[str(options)]['intermediate_ranking']}\")\n",
    "    print(f\"Non-Expert Rankings: {rankings[str(options)]['nonexpert_ranking']}\")\n",
    "    print()\n",
    "\n",
    "    idata = az.from_pystan(posterior=synth_fit)\n",
    "    models[str(options)] = idata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c7c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mean_params(fit):\n",
    "    summary = fit.summary()\n",
    "    param_names = summary['summary_rownames']\n",
    "    means = summary['summary'][:, 0]  # First column is the mean\n",
    "    return dict(zip(param_names, means))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2ff586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comparison_records = []\n",
    "\n",
    "for key in fits.keys():\n",
    "    print(f\"\\nüìä Comparing parameter recovery for option set: {key}\")\n",
    "    try:\n",
    "        original_params = extract_mean_params(fits[key])\n",
    "        synthetic_params = extract_mean_params(synth_fits[key])\n",
    "\n",
    "        # Determine the number of options (N) from any strength param group\n",
    "        N = sum(1 for p in original_params if p.startswith('strengths_expert_vote'))\n",
    "\n",
    "        # Compare mixing proportions\n",
    "        for i in range(1, 4):  # pi[1], pi[2], pi[3]\n",
    "            param = f'pi[{i}]'\n",
    "            rel_error = abs(original_params[param] - synthetic_params[param]) / original_params[param]\n",
    "            comparison_records.append({\n",
    "                'options': key,\n",
    "                'parameter': param,\n",
    "                'original': original_params[param],\n",
    "                'synthetic': synthetic_params[param],\n",
    "                'relative_error': rel_error\n",
    "            })\n",
    "            print(f\"{param}: Relative Error = {rel_error:.4f}\")\n",
    "\n",
    "        # Compare all strength parameters\n",
    "        for group in ['expert_vote', 'intermediate_vote', 'nonexpert_vote',\n",
    "                      'expert_prediction', 'intermediate_prediction', 'nonexpert_prediction']:\n",
    "            for i in range(N):\n",
    "                param = f'strengths_{group}[{i+1}]'\n",
    "                rel_error = abs(original_params[param] - synthetic_params[param]) / original_params[param]\n",
    "                comparison_records.append({\n",
    "                    'options': key,\n",
    "                    'parameter': param,\n",
    "                    'original': original_params[param],\n",
    "                    'synthetic': synthetic_params[param],\n",
    "                    'relative_error': rel_error\n",
    "                })\n",
    "                print(f\"{param}: Relative Error = {rel_error:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not compare for {key}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac18b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = pd.DataFrame(comparison_records)\n",
    "df_compare.to_csv('parameter_recovery_comparison_3groups.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d881ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize dictionaries to store relative errors for each parameter group\n",
    "relative_errors = {\n",
    "    'expert_vote': [],\n",
    "    'intermediate_vote': [],\n",
    "    'nonexpert_vote': [],\n",
    "    'expert_prediction': [],\n",
    "    'intermediate_prediction': [],\n",
    "    'nonexpert_prediction': [],\n",
    "    'expert_proportion': [],\n",
    "    'intermediate_proportion': [],\n",
    "    'nonexpert_proportion': []\n",
    "}\n",
    "\n",
    "# Iterate over each group of options\n",
    "for key in fits.keys():\n",
    "    try:\n",
    "        original_params = extract_mean_params(fits[key])\n",
    "        synthetic_params = extract_mean_params(synth_fits[key])\n",
    "\n",
    "        # Determine the number of options (N)\n",
    "        N = sum(1 for p in original_params if p.startswith('strengths_expert_vote'))\n",
    "\n",
    "        # Compute relative errors for each parameter group\n",
    "        for i in range(N):\n",
    "            for group in ['expert', 'intermediate', 'nonexpert']:\n",
    "                for mode in ['vote', 'prediction']:\n",
    "                    param = f'strengths_{group}_{mode}[{i+1}]'\n",
    "                    rel_error = abs(original_params[param] - synthetic_params[param]) / original_params[param]\n",
    "                    relative_errors[f'{group}_{mode}'].append(rel_error)\n",
    "\n",
    "        # Group proportions\n",
    "        for i, group in enumerate(['expert', 'intermediate', 'nonexpert']):\n",
    "            param = f'pi[{i+1}]'\n",
    "            rel_error = abs(original_params[param] - synthetic_params[param]) / original_params[param]\n",
    "            relative_errors[f'{group}_proportion'].append(rel_error)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not compare for {key}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49471d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average relative errors\n",
    "average_errors = {param: np.mean(errors) for param, errors in relative_errors.items()}\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nüìä Aggregate Parameter Recovery Errors:\")\n",
    "for param_group, avg_error in average_errors.items():\n",
    "    print(f\"üîπ {param_group.replace('_', ' ').title()}: {avg_error:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d17c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results to a DataFrame\n",
    "df_summary = pd.DataFrame([\n",
    "    {'Parameter Group': param_group.replace('_', ' ').title(), 'Average Relative Error': avg_error}\n",
    "    for param_group, avg_error in average_errors.items()\n",
    "])\n",
    "\n",
    "# Save to CSV\n",
    "df_summary.to_csv('aggregate_parameter_recovery_summary_3.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stan_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec3516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% import pandas as pd\n",
    "import numpy as np\n",
    "import pystan\n",
    "from scipy.stats import kendalltau\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Elicitation Formats/rank-rank/rank-rank_1_4.csv')  # Replace with your actual filename\n",
    "\n",
    "# %% Convert string representations of lists to lists\n",
    "import ast\n",
    "df['options'] = df['options'].apply(ast.literal_eval)\n",
    "df['votes'] = df['votes'].apply(ast.literal_eval)\n",
    "df['predictions'] = df['predictions'].apply(ast.literal_eval)\n",
    "\n",
    "# Constants\n",
    "N = 10  # Number of voters\n",
    "J = df['questions'].nunique()  # Number of questions\n",
    "K = len(df['options'].iloc[0])  # Number of options per vote (assuming uniform)\n",
    "\n",
    "# Calculate Kendall tau distances\n",
    "kendall_tau_votes = np.zeros((N, J))\n",
    "kendall_tau_predictions = np.zeros((N, J))\n",
    "\n",
    "for j in range(J):\n",
    "    question_responses = df[df['questions'] == j + 1]\n",
    "    for i, response in question_responses.iterrows():\n",
    "        kt_vote = 1-kendalltau(response['votes'], response['options'])[0]\n",
    "        kt_prediction = 1-kendalltau(response['predictions'], response['options'])[0]\n",
    "        n = i % N \n",
    "        kendall_tau_votes[n, j] = kt_vote\n",
    "        kendall_tau_predictions[n, j] = kt_prediction\n",
    "\n",
    "# Stan Data\n",
    "stan_data = {\n",
    "    'N': N,\n",
    "    'K': K,\n",
    "    'J': J,\n",
    "    'kendall_tau_votes': kendall_tau_votes,\n",
    "    'kendall_tau_predictions': kendall_tau_predictions,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9992459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_with_priors(stan_data, priors):\n",
    "    stan_model_code = f\"\"\"\n",
    "    data {{\n",
    "      int<lower=1> N;\n",
    "      int<lower=1> J;\n",
    "      real kendall_tau_votes[N, J];\n",
    "      real kendall_tau_predictions[N, J];\n",
    "      vector[2] alpha;  // Alpha as data\n",
    "    }}\n",
    "\n",
    "    parameters {{\n",
    "      real<lower=0> dispersion_vote_expert;\n",
    "      real<lower=0> dispersion_vote_nonexpert;\n",
    "      real<lower=0> dispersion_pred_expert;\n",
    "      real<lower=0> dispersion_pred_nonexpert;\n",
    "      simplex[2] prob_group;\n",
    "    }}\n",
    "\n",
    "    model {{\n",
    "      dispersion_vote_expert ~ normal({priors['vote_expert'][0]}, {priors['vote_expert'][1]});\n",
    "      dispersion_vote_nonexpert ~ normal({priors['vote_nonexpert'][0]}, {priors['vote_nonexpert'][1]});\n",
    "      dispersion_pred_expert ~ normal({priors['pred_expert'][0]}, {priors['pred_expert'][1]});\n",
    "      dispersion_pred_nonexpert ~ normal({priors['pred_nonexpert'][0]}, {priors['pred_nonexpert'][1]});\n",
    "      \n",
    "      prob_group ~ dirichlet(alpha);  // Use alpha from data\n",
    "\n",
    "      for (n in 1:N) {{\n",
    "         vector[2] log_lik;\n",
    "         log_lik[1] = normal_lpdf(kendall_tau_votes[n] | 0, dispersion_vote_expert) + \n",
    "                      normal_lpdf(kendall_tau_predictions[n] | 0, dispersion_pred_expert);\n",
    "         log_lik[2] = normal_lpdf(kendall_tau_votes[n] | 0, dispersion_vote_nonexpert) + \n",
    "                      normal_lpdf(kendall_tau_predictions[n] | 0, dispersion_pred_nonexpert);\n",
    "         target += log_sum_exp(log_lik + log(prob_group));\n",
    "       }}\n",
    "     }}\n",
    "   generated quantities {{\n",
    "        vector[N] log_lik;\n",
    "        for (n in 1:N) {{\n",
    "            vector[2] log_lik_n;\n",
    "            log_lik_n[1] = normal_lpdf(kendall_tau_votes[n] | 0, dispersion_vote_expert) +\n",
    "                           normal_lpdf(kendall_tau_predictions[n] | 0, dispersion_pred_expert);\n",
    "            log_lik_n[2] = normal_lpdf(kendall_tau_votes[n] | 0, dispersion_vote_nonexpert) +\n",
    "                           normal_lpdf(kendall_tau_predictions[n] | 0, dispersion_pred_nonexpert);\n",
    "            log_lik[n] = log_sum_exp(log_lik_n + log(prob_group));\n",
    "        }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert alpha list to a numpy array for Stan\n",
    "    alpha_array = np.array([priors['alpha'][0], priors['alpha'][1]])\n",
    "\n",
    "    # Include alpha in the data dictionary\n",
    "    stan_data['alpha'] = alpha_array\n",
    "\n",
    "    sm = pystan.StanModel(model_code=stan_model_code)\n",
    "    fit = sm.sampling(data=stan_data, \n",
    "                      iter=8000,\n",
    "                      warmup=2000,\n",
    "                      chains=4,\n",
    "                      control={\n",
    "                          'adapt_delta': 0.95,\n",
    "                          'max_treedepth': 15\n",
    "                      })\n",
    "    return fit\n",
    "\n",
    "# Define different sets of priors\n",
    "prior_sets = {\n",
    "    'case1': {\n",
    "        'vote_expert': (0.1, 0.2),\n",
    "        'vote_nonexpert': (0.8, 0.2),\n",
    "        'pred_expert': (0.4, 0.3),\n",
    "        'pred_nonexpert': (0.8, 0.3),\n",
    "        'alpha':[2, 4]\n",
    "    }\n",
    "}\n",
    "# Run models with different priors\n",
    "results = {}\n",
    "for prior_name, priors in prior_sets.items():\n",
    "    print(f\"Running model with {prior_name} priors...\")\n",
    "    results[prior_name] = run_model_with_priors(stan_data, priors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5530bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract learned parameters from Stan output\n",
    "summary = results['case1'].summary()\n",
    "param_names = summary['summary_rownames']\n",
    "posterior_means = summary['summary'][:, 0]\n",
    "param_mean_dict = dict(zip(param_names, posterior_means))\n",
    "\n",
    "# Set parameters\n",
    "learned_params = {\n",
    "    'dispersion_vote_expert': param_mean_dict['dispersion_vote_expert'],\n",
    "    'dispersion_vote_nonexpert': param_mean_dict['dispersion_vote_nonexpert'],\n",
    "    'dispersion_pred_expert': param_mean_dict['dispersion_pred_expert'],\n",
    "    'dispersion_pred_nonexpert': param_mean_dict['dispersion_pred_nonexpert'],\n",
    "    'prob_expert': param_mean_dict['prob_group[1]'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c432b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "from itertools import permutations\n",
    "from scipy.stats import kendalltau\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Vote:\n",
    "    def __init__(self, question_number: int, options: List[int], ranking: List[int], predicted_probs: dict, is_expert: bool):\n",
    "        self.question_number = question_number\n",
    "        self.options = options\n",
    "        self.ranking = ranking\n",
    "        self.predicted_probs = predicted_probs\n",
    "        self.is_expert = is_expert\n",
    "\n",
    "class Voter:\n",
    "    def __init__(self, is_expert: bool, params):\n",
    "        self.is_expert = is_expert\n",
    "        self.params = params \n",
    "    def vote(self, question_number: int, options: List[int], ground_truth: List[int], all_worlds: List[Tuple[int]]):\n",
    "\n",
    "        if self.is_expert:\n",
    "            centroid = ground_truth\n",
    "        else:\n",
    "            centroid = random.choice(all_worlds)\n",
    "\n",
    "        # Compute the Mallows probabilities for all possible signals\n",
    "        signal_probs = {}\n",
    "        for possible_signal in all_worlds:\n",
    "            signal_probs[possible_signal] = signal_probability(possible_signal, centroid, self.is_expert, self.params, mode='vote')\n",
    "\n",
    "        # Normalize the probabilities so they sum to 1\n",
    "        total_prob = sum(signal_probs.values())\n",
    "        normalized_signal_probs = {signal: prob/total_prob for signal, prob in signal_probs.items()}\n",
    "\n",
    "        # Choose the signal with the maximum probability\n",
    "        signal = max(normalized_signal_probs, key=normalized_signal_probs.get)\n",
    "\n",
    "\n",
    "        # Compute the conditional probabilities and the predicted ranking\n",
    "        conditional_probs = {}\n",
    "        for possible_signal in all_worlds: \n",
    "            s_j = possible_signal\n",
    "            s_k = signal\n",
    "            conditional_probs[s_j] = compute_conditional_prob(s_j, s_k, all_worlds, ground_truth, self.is_expert, self.params, mode='pred')\n",
    "\n",
    "        # Call the predict method to get the predicted probabilities\n",
    "        predicted_probs = self.predict(signal, conditional_probs, all_worlds, ground_truth)\n",
    "\n",
    "        # Determine prediction by finding the ranking with the highest predicted probability\n",
    "        prediction = max(predicted_probs, key=predicted_probs.get)\n",
    "\n",
    "        return Vote(question_number, options, signal, prediction, self.is_expert)\n",
    "\n",
    "    def predict(self, signal, conditional_probs, all_worlds, ground_truth):\n",
    "        # Prepare the prediction probabilities.\n",
    "        prediction_probs = {world: prob for world, prob in conditional_probs.items() if world != signal}\n",
    "\n",
    "        # Normalize the probabilities so that they sum to 1.\n",
    "        total_prob = sum(prediction_probs.values())\n",
    "        normalized_prediction_probs = {world: prob/total_prob for world, prob in prediction_probs.items()}\n",
    "\n",
    "        return normalized_prediction_probs\n",
    "\n",
    "\n",
    "def mallows_distance(ranking1, ranking2):\n",
    "    tau, _ = kendalltau(ranking1, ranking2)\n",
    "    return 1 - tau  # Inverting tau to represent a 'distance'\n",
    "\n",
    "def normalization_constant(phi, m):\n",
    "    z = 1\n",
    "    for i in range(1, m):\n",
    "        z *= sum(phi**j for j in range(i + 1))\n",
    "    return z\n",
    "\n",
    "def signal_probability(signal, world, is_expert, params, mode='vote'):\n",
    "    if mode == 'vote':\n",
    "        dispersion = params['dispersion_vote_expert'] if is_expert else params['dispersion_vote_nonexpert']\n",
    "    else:\n",
    "        dispersion = params['dispersion_pred_expert'] if is_expert else params['dispersion_pred_nonexpert']\n",
    "\n",
    "    distance = mallows_distance(signal, world)\n",
    "    m = len(signal)\n",
    "    phi = dispersion\n",
    "    prob = phi**distance / normalization_constant(phi, m)\n",
    "    return prob\n",
    "\n",
    "\n",
    "computed_posteriors = {}\n",
    "\n",
    "def compute_posterior(signal, world, all_worlds, ground_truth, is_expert, params, mode='vote'):\n",
    "    key = (tuple(signal), world, tuple(ground_truth), is_expert, mode)\n",
    "    if key in computed_posteriors:\n",
    "        return computed_posteriors[key]\n",
    "    \n",
    "    prior = 1 / len(all_worlds)\n",
    "    likelihood = signal_probability(signal, world, is_expert, params, mode=mode)\n",
    "    total_signal_prob = sum(signal_probability(signal, w, is_expert, params, mode=mode) * prior for w in all_worlds)\n",
    "    posterior = likelihood * prior / total_signal_prob\n",
    "    computed_posteriors[key] = posterior\n",
    "    return posterior\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_conditional_prob(s_j, s_k, all_worlds, ground_truth, is_expert, params, mode='vote', num_samples=1000):\n",
    "    weights = [compute_posterior(s_k, w, all_worlds, ground_truth, is_expert, params, mode=mode) for w in all_worlds]\n",
    "    sampled_worlds = random.choices(all_worlds, weights=weights, k=num_samples)\n",
    "\n",
    "    total_prob = 0.0\n",
    "    for world_i in sampled_worlds:\n",
    "        p_sj_wi = signal_probability(s_j, world_i, is_expert, params, mode=mode)\n",
    "        total_prob += p_sj_wi\n",
    "\n",
    "    return total_prob / num_samples\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def simulate_voting(num_voters: int, subset: List[int], ground_truth: List[int], params) -> Tuple[List[Vote]]:\n",
    "    prob_expert = params['prob_expert']\n",
    "    num_experts = np.random.binomial(num_voters, prob_expert)\n",
    "\n",
    "    # Initialize voter list\n",
    "    voters = [Voter(is_expert=False, params=params) for _ in range(num_voters)]\n",
    "\n",
    "    # Randomly assign experts by selecting indices at random\n",
    "    expert_indices = np.random.choice(num_voters, size=num_experts, replace=False)\n",
    "\n",
    "    for idx in expert_indices:\n",
    "        voters[idx].is_expert = True\n",
    "\n",
    "    # Count experts and non-experts in every 100 voters\n",
    "    for i in range(100, num_voters + 1, 100):\n",
    "        experts_in_chunk = sum(voter.is_expert for voter in voters[i-100:i])\n",
    "        non_experts_in_chunk = 100 - experts_in_chunk\n",
    "        print(f\"In samples {i-99} to {i}: Experts = {experts_in_chunk}, Non-experts = {non_experts_in_chunk}\")\n",
    "\n",
    "    \n",
    "    votes = []\n",
    "    all_worlds = all_worlds = list(permutations(subset))\n",
    "    question_number=1 \n",
    "\n",
    "    for voter in tqdm(voters, desc=\"Simulating Votes\"):\n",
    "        vote = voter.vote(question_number, subset, ground_truth, all_worlds)\n",
    "        votes.append(vote)\n",
    "\n",
    "    return votes\n",
    "\n",
    "\n",
    "def write_to_csv(votes):\n",
    "    data = []\n",
    "    for vote in votes:\n",
    "        data.append([vote.question_number, vote.options, vote.ranking, vote.predicted_probs, vote.is_expert, 1, 6])\n",
    "        \n",
    "    df = pd.DataFrame(data, columns=['questions', 'options', 'votes', 'predictions', 'is_expert', 'domain', 'treatment'])\n",
    "    df.to_csv('groups2_mallows_simulated_data.csv', index=False)\n",
    "\n",
    "\n",
    "# Test the simulation\n",
    "num_voters = 1000\n",
    "m = 5 # Total number of alternatives\n",
    "\n",
    "m = [i for i in range(1,m+1)]\n",
    "# Generate ground truths for each subset. In this example, we assume the ground truth \n",
    "# is the options sorted in ascending order. You can replace this with your actual ground truths.\n",
    "ground_truth = sorted(m)\n",
    "\n",
    "votes = simulate_voting(num_voters, m, ground_truth, learned_params)\n",
    "\n",
    "write_to_csv(votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e5fbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "# Load the simulated dataset\n",
    "df_sim = pd.read_csv('groups2_mallows_simulated_data.csv')\n",
    "\n",
    "# Convert string fields to lists\n",
    "df_sim['options'] = df_sim['options'].apply(ast.literal_eval)\n",
    "df_sim['votes'] = df_sim['votes'].apply(ast.literal_eval)\n",
    "df_sim['predictions'] = df_sim['predictions'].apply(ast.literal_eval)\n",
    "\n",
    "# Constants\n",
    "N = 10  # Or however many you want to subsample\n",
    "J = df_sim['questions'].nunique()\n",
    "K = len(df_sim['options'].iloc[0])  # number of items\n",
    "\n",
    "# Initialize matrices\n",
    "kendall_tau_votes = np.zeros((N, J))\n",
    "kendall_tau_predictions = np.zeros((N, J))\n",
    "\n",
    "# Build matrices\n",
    "for j in range(J):\n",
    "    question_responses = df_sim[df_sim['questions'] == j + 1].head(N)  # limit to N\n",
    "    for i, response in question_responses.iterrows():\n",
    "        n = i % N\n",
    "        kt_vote = 1 - kendalltau(response['votes'], response['options'])[0]\n",
    "        kt_prediction = 1 - kendalltau(response['predictions'], response['options'])[0]\n",
    "        kendall_tau_votes[n, j] = kt_vote\n",
    "        kendall_tau_predictions[n, j] = kt_prediction\n",
    "\n",
    "# Build Stan data dict\n",
    "stan_data_synth = {\n",
    "    'N': N,\n",
    "    'K': K,\n",
    "    'J': J,\n",
    "    'kendall_tau_votes': kendall_tau_votes,\n",
    "    'kendall_tau_predictions': kendall_tau_predictions,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414ef7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_synth = run_model_with_priors(stan_data_synth, prior_sets['case1'])\n",
    "results['synth_case1'] = fit_synth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fc1035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mean_params(fit):\n",
    "    summary = fit.summary()\n",
    "    param_names = summary['summary_rownames']\n",
    "    means = summary['summary'][:, 0]\n",
    "    return dict(zip(param_names, means))\n",
    "\n",
    "# Extract means\n",
    "original_params = extract_mean_params(results['case1'])\n",
    "synth_params = extract_mean_params(results['synth_case1'])\n",
    "\n",
    "# Collect results\n",
    "comparison_records = []\n",
    "\n",
    "for key in ['dispersion_vote_expert', 'dispersion_vote_nonexpert',\n",
    "            'dispersion_pred_expert', 'dispersion_pred_nonexpert',\n",
    "            'prob_group[1]', 'prob_group[2]']:\n",
    "    rel_error = abs(original_params[key] - synth_params[key]) / original_params[key]\n",
    "    comparison_records.append({\n",
    "        'parameter': key,\n",
    "        'original': original_params[key],\n",
    "        'synthetic': synth_params[key],\n",
    "        'relative_error': rel_error\n",
    "    })\n",
    "    print(f\"{key}: Relative Error = {rel_error:.4f}\")\n",
    "\n",
    "# Save to CSV\n",
    "df_compare = pd.DataFrame(comparison_records)\n",
    "df_compare.to_csv('group2_parameter_recovery_comparison_mallows.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c540922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% import pandas as pd\n",
    "import numpy as np\n",
    "import pystan\n",
    "from scipy.stats import kendalltau\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Elicitation Formats/rank-rank/rank-rank_1_4.csv')\n",
    "\n",
    "# %% Convert string representations of lists to lists\n",
    "import ast\n",
    "df['options'] = df['options'].apply(ast.literal_eval)\n",
    "df['votes'] = df['votes'].apply(ast.literal_eval)\n",
    "df['predictions'] = df['predictions'].apply(ast.literal_eval)\n",
    "\n",
    "# Constants\n",
    "N = 10  # Number of voters\n",
    "J = df['questions'].nunique()  # Number of questions\n",
    "K = len(df['options'].iloc[0])  # Number of options per vote (assuming uniform)\n",
    "\n",
    "# Calculate Kendall tau distances\n",
    "kendall_tau_votes = np.zeros((N, J))\n",
    "kendall_tau_predictions = np.zeros((N, J))\n",
    "\n",
    "for j in range(J):\n",
    "    question_responses = df[df['questions'] == j + 1]\n",
    "    for i, response in question_responses.iterrows():\n",
    "        kt_vote = 1-kendalltau(response['votes'], response['options'])[0]\n",
    "        kt_prediction = 1-kendalltau(response['predictions'], response['options'])[0]\n",
    "        n = i % N \n",
    "        kendall_tau_votes[n, j] = kt_vote\n",
    "        kendall_tau_predictions[n, j] = kt_prediction\n",
    "\n",
    "# Stan Data\n",
    "stan_data = {\n",
    "    'N': N,\n",
    "    'K': K,\n",
    "    'J': J,\n",
    "    'kendall_tau_votes': kendall_tau_votes,\n",
    "    'kendall_tau_predictions': kendall_tau_predictions,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4f0624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_with_priors(stan_data, priors):\n",
    "    stan_model_code = f\"\"\"\n",
    "    data {{\n",
    "      int<lower=1> N;\n",
    "      int<lower=1> J;\n",
    "      real kendall_tau_votes[N, J];\n",
    "      real kendall_tau_predictions[N, J];\n",
    "      vector[3] alpha;  // Alpha as data\n",
    "    }}\n",
    "\n",
    "    parameters {{\n",
    "      real<lower=0> dispersion_vote_expert;\n",
    "      real<lower=0> dispersion_vote_intermediate;\n",
    "      real<lower=0> dispersion_vote_nonexpert;\n",
    "      real<lower=0> dispersion_pred_expert;\n",
    "      real<lower=0> dispersion_pred_intermediate;\n",
    "      real<lower=0> dispersion_pred_nonexpert;\n",
    "      simplex[3] prob_group;\n",
    "    }}\n",
    "\n",
    "    model {{\n",
    "      dispersion_vote_expert ~ normal({priors['vote_expert'][0]}, {priors['vote_expert'][1]});\n",
    "      dispersion_vote_intermediate ~ normal({priors['vote_intermediate'][0]}, {priors['vote_intermediate'][1]});\n",
    "      dispersion_vote_nonexpert ~ normal({priors['vote_nonexpert'][0]}, {priors['vote_nonexpert'][1]});\n",
    "      dispersion_pred_expert ~ normal({priors['pred_expert'][0]}, {priors['pred_expert'][1]});\n",
    "      dispersion_pred_intermediate ~ normal({priors['pred_intermediate'][0]}, {priors['pred_intermediate'][1]});\n",
    "      dispersion_pred_nonexpert ~ normal({priors['pred_nonexpert'][0]}, {priors['pred_nonexpert'][1]});\n",
    "      \n",
    "      prob_group ~ dirichlet(alpha);  // Use alpha from data\n",
    "\n",
    "      for (n in 1:N) {{\n",
    "         vector[3] log_lik;\n",
    "         log_lik[1] = normal_lpdf(kendall_tau_votes[n] | 0, dispersion_vote_expert) + \n",
    "                      normal_lpdf(kendall_tau_predictions[n] | 0, dispersion_pred_expert);\n",
    "         log_lik[2] = normal_lpdf(kendall_tau_votes[n] | 0, dispersion_vote_intermediate) + \n",
    "                      normal_lpdf(kendall_tau_predictions[n] | 0, dispersion_pred_intermediate);\n",
    "         log_lik[3] = normal_lpdf(kendall_tau_votes[n] | 0, dispersion_vote_nonexpert) + \n",
    "                      normal_lpdf(kendall_tau_predictions[n] | 0, dispersion_pred_nonexpert);\n",
    "         target += log_sum_exp(log_lik + log(prob_group));\n",
    "       }}\n",
    "     }}\n",
    "   generated quantities {{\n",
    "        vector[N] log_lik;\n",
    "        for (n in 1:N) {{\n",
    "            vector[3] log_lik_n;\n",
    "            log_lik_n[1] = normal_lpdf(kendall_tau_votes[n] | 0, dispersion_vote_expert) +\n",
    "                           normal_lpdf(kendall_tau_predictions[n] | 0, dispersion_pred_expert);\n",
    "            log_lik_n[2] = normal_lpdf(kendall_tau_votes[n] | 0, dispersion_vote_intermediate) +\n",
    "                           normal_lpdf(kendall_tau_predictions[n] | 0, dispersion_pred_intermediate);\n",
    "            log_lik_n[3] = normal_lpdf(kendall_tau_votes[n] | 0, dispersion_vote_nonexpert) +\n",
    "                           normal_lpdf(kendall_tau_predictions[n] | 0, dispersion_pred_nonexpert);\n",
    "            log_lik[n] = log_sum_exp(log_lik_n + log(prob_group));\n",
    "        }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert alpha list to a numpy array for Stan\n",
    "    alpha_array = np.array([priors['alpha'][0], priors['alpha'][1], priors['alpha'][2]])\n",
    "\n",
    "    # Include alpha in the data dictionary\n",
    "    stan_data['alpha'] = alpha_array\n",
    "\n",
    "    sm = pystan.StanModel(model_code=stan_model_code)\n",
    "    fit = sm.sampling(data=stan_data, \n",
    "                      iter=8000,\n",
    "                      warmup=2000,\n",
    "                      chains=4,\n",
    "                      control={\n",
    "                          'adapt_delta': 0.95,\n",
    "                          'max_treedepth': 15\n",
    "                      })\n",
    "    return fit\n",
    "\n",
    "\n",
    "prior_sets = {\n",
    "    'case1': {\n",
    "        'vote_expert': (0.1, 0.2),\n",
    "        'vote_intermediate': (0.4, 0.2),\n",
    "        'vote_nonexpert': (0.8, 0.2),\n",
    "        'pred_expert': (0.4, 0.3),\n",
    "        'pred_intermediate': (0.4, 0.3),\n",
    "        'pred_nonexpert': (0.8, 0.3),\n",
    "        'alpha':[2, 2, 4]\n",
    "    }\n",
    "}\n",
    "# Run models with different priors\n",
    "results = {}\n",
    "for prior_name, priors in prior_sets.items():\n",
    "    print(f\"Running model with {prior_name} priors...\")\n",
    "    results[prior_name] = run_model_with_priors(stan_data, priors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec240289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract learned parameters from Stan output\n",
    "summary = results['case1'].summary()\n",
    "param_names = summary['summary_rownames']\n",
    "posterior_means = summary['summary'][:, 0]\n",
    "param_mean_dict = dict(zip(param_names, posterior_means))\n",
    "\n",
    "# Set parameters for 3-group setting\n",
    "learned_params = {\n",
    "    'dispersion_vote_expert': param_mean_dict['dispersion_vote_expert'],\n",
    "    'dispersion_vote_intermediate': param_mean_dict['dispersion_vote_intermediate'],\n",
    "    'dispersion_vote_nonexpert': param_mean_dict['dispersion_vote_nonexpert'],\n",
    "    'dispersion_pred_expert': param_mean_dict['dispersion_pred_expert'],\n",
    "    'dispersion_pred_intermediate': param_mean_dict['dispersion_pred_intermediate'],\n",
    "    'dispersion_pred_nonexpert': param_mean_dict['dispersion_pred_nonexpert'],\n",
    "    'prob_expert': param_mean_dict['prob_group[1]'],\n",
    "    'prob_intermediate': param_mean_dict['prob_group[2]'],\n",
    "    'prob_nonexpert': param_mean_dict['prob_group[3]'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c57176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "from itertools import permutations\n",
    "from scipy.stats import kendalltau\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Vote:\n",
    "    def __init__(self, question_number: int, options: List[int], ranking: List[int], predicted_probs: dict, is_expert: bool):\n",
    "        self.question_number = question_number\n",
    "        self.options = options\n",
    "        self.ranking = ranking\n",
    "        self.predicted_probs = predicted_probs\n",
    "        self.is_expert = is_expert\n",
    "\n",
    "class Voter:\n",
    "    def __init__(self, group: int, params):\n",
    "        self.group = group  # 0 = expert, 1 = intermediate, 2 = non-expert\n",
    "        self.params = params\n",
    "\n",
    "    def vote(self, question_number: int, options: List[int], ground_truth: List[int], all_worlds: List[Tuple[int]]):\n",
    "\n",
    "        centroid = ground_truth if self.group == 0 else random.choice(all_worlds)\n",
    "\n",
    "        # Mallows probabilities for signals\n",
    "        signal_probs = {\n",
    "            signal: signal_probability(signal, centroid, self.group, self.params, mode='vote')\n",
    "            for signal in all_worlds\n",
    "        }\n",
    "\n",
    "        # Normalize\n",
    "        total_prob = sum(signal_probs.values())\n",
    "        normalized_signal_probs = {signal: prob / total_prob for signal, prob in signal_probs.items()}\n",
    "        signal = max(normalized_signal_probs, key=normalized_signal_probs.get)\n",
    "\n",
    "        # Conditional probabilities\n",
    "        conditional_probs = {\n",
    "            s_j: compute_conditional_prob(s_j, signal, all_worlds, ground_truth, self.group, self.params, mode='pred')\n",
    "            for s_j in all_worlds\n",
    "        }\n",
    "\n",
    "        predicted_probs = self.predict(signal, conditional_probs, all_worlds, ground_truth)\n",
    "        prediction = max(predicted_probs, key=predicted_probs.get)\n",
    "\n",
    "        return Vote(question_number, options, signal, prediction, self.group)\n",
    "\n",
    "\n",
    "    def predict(self, signal, conditional_probs, all_worlds, ground_truth):\n",
    "        # Prepare the prediction probabilities.\n",
    "        prediction_probs = {world: prob for world, prob in conditional_probs.items() if world != signal}\n",
    "\n",
    "        # Normalize the probabilities so that they sum to 1.\n",
    "        total_prob = sum(prediction_probs.values())\n",
    "        normalized_prediction_probs = {world: prob/total_prob for world, prob in prediction_probs.items()}\n",
    "\n",
    "        return normalized_prediction_probs\n",
    "\n",
    "\n",
    "def mallows_distance(ranking1, ranking2):\n",
    "    tau, _ = kendalltau(ranking1, ranking2)\n",
    "    return 1 - tau  # Inverting tau to represent a 'distance'\n",
    "\n",
    "def normalization_constant(phi, m):\n",
    "    z = 1\n",
    "    for i in range(1, m):\n",
    "        z *= sum(phi**j for j in range(i + 1))\n",
    "    return z\n",
    "\n",
    "def signal_probability(signal, world, group, params, mode='vote'):\n",
    "    key = f'dispersion_{mode}_expert' if group == 0 else (\n",
    "          f'dispersion_{mode}_intermediate' if group == 1 else f'dispersion_{mode}_nonexpert')\n",
    "    \n",
    "    dispersion = params[key]\n",
    "    distance = mallows_distance(signal, world)\n",
    "    m = len(signal)\n",
    "    phi = dispersion\n",
    "    prob = phi**distance / normalization_constant(phi, m)\n",
    "    return prob\n",
    "\n",
    "\n",
    "\n",
    "computed_posteriors = {}\n",
    "\n",
    "def compute_posterior(signal, world, all_worlds, ground_truth, is_expert, params, mode='vote'):\n",
    "    key = (tuple(signal), world, tuple(ground_truth), is_expert, mode)\n",
    "    if key in computed_posteriors:\n",
    "        return computed_posteriors[key]\n",
    "    \n",
    "    prior = 1 / len(all_worlds)\n",
    "    likelihood = signal_probability(signal, world, is_expert, params, mode=mode)\n",
    "    total_signal_prob = sum(signal_probability(signal, w, is_expert, params, mode=mode) * prior for w in all_worlds)\n",
    "    posterior = likelihood * prior / total_signal_prob\n",
    "    computed_posteriors[key] = posterior\n",
    "    return posterior\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_conditional_prob(s_j, s_k, all_worlds, ground_truth, is_expert, params, mode='vote', num_samples=1000):\n",
    "    weights = [compute_posterior(s_k, w, all_worlds, ground_truth, is_expert, params, mode=mode) for w in all_worlds]\n",
    "    sampled_worlds = random.choices(all_worlds, weights=weights, k=num_samples)\n",
    "\n",
    "    total_prob = 0.0\n",
    "    for world_i in sampled_worlds:\n",
    "        p_sj_wi = signal_probability(s_j, world_i, is_expert, params, mode=mode)\n",
    "        total_prob += p_sj_wi\n",
    "\n",
    "    return total_prob / num_samples\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def simulate_voting(num_voters: int, subset: List[int], ground_truth: List[int], params) -> List[Vote]:\n",
    "    prob_expert = params['prob_expert']\n",
    "    prob_intermediate = params['prob_intermediate']\n",
    "    prob_nonexpert = params['prob_nonexpert']\n",
    "\n",
    "    group_assignments = np.random.choice(\n",
    "        [0, 1, 2],\n",
    "        size=num_voters,\n",
    "        p=[prob_expert, prob_intermediate, prob_nonexpert]\n",
    "    )\n",
    "\n",
    "    voters = [Voter(group=g, params=params) for g in group_assignments]\n",
    "\n",
    "    # Optional: group count summary\n",
    "    for i in range(100, num_voters + 1, 100):\n",
    "        chunk = voters[i-100:i]\n",
    "        counts = [sum(v.group == g for v in chunk) for g in [0, 1, 2]]\n",
    "        print(f\"Samples {i-99}-{i} âž¤ Experts: {counts[0]}, Intermediates: {counts[1]}, Non-experts: {counts[2]}\")\n",
    "\n",
    "    all_worlds = list(permutations(subset))\n",
    "    question_number = 1\n",
    "\n",
    "    votes = []\n",
    "    for voter in tqdm(voters, desc=\"Simulating Votes\"):\n",
    "        vote = voter.vote(question_number, subset, ground_truth, all_worlds)\n",
    "        votes.append(vote)\n",
    "\n",
    "    return votes\n",
    "\n",
    "\n",
    "\n",
    "def write_to_csv(votes):\n",
    "    data = []\n",
    "    for vote in votes:\n",
    "        data.append([vote.question_number, vote.options, vote.ranking, vote.predicted_probs, vote.is_expert, 1, 6])\n",
    "        \n",
    "    df = pd.DataFrame(data, columns=['questions', 'options', 'votes', 'predictions', 'group', 'domain', 'treatment'])\n",
    "    df.to_csv('groups3_mallows_simulated_data.csv', index=False)\n",
    "\n",
    "\n",
    "# Test the simulation\n",
    "num_voters = 1000\n",
    "m = 5 # Total number of alternatives\n",
    "\n",
    "m = [i for i in range(1,m+1)]\n",
    "# Generate ground truths for each subset. In this example, we assume the ground truth \n",
    "# is the options sorted in ascending order. You can replace this with your actual ground truths.\n",
    "ground_truth = sorted(m)\n",
    "\n",
    "votes = simulate_voting(num_voters, m, ground_truth, learned_params)\n",
    "\n",
    "write_to_csv(votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5c5d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "# Load the simulated dataset\n",
    "df_sim = pd.read_csv('groups3_mallows_simulated_data.csv')\n",
    "\n",
    "# Convert string fields to lists\n",
    "df_sim['options'] = df_sim['options'].apply(ast.literal_eval)\n",
    "df_sim['votes'] = df_sim['votes'].apply(ast.literal_eval)\n",
    "df_sim['predictions'] = df_sim['predictions'].apply(ast.literal_eval)\n",
    "\n",
    "# Constants\n",
    "N = 10  # Or however many you want to subsample\n",
    "J = df_sim['questions'].nunique()\n",
    "K = len(df_sim['options'].iloc[0])  # number of items\n",
    "\n",
    "# Initialize matrices\n",
    "kendall_tau_votes = np.zeros((N, J))\n",
    "kendall_tau_predictions = np.zeros((N, J))\n",
    "\n",
    "# Build matrices\n",
    "for j in range(J):\n",
    "    question_responses = df_sim[df_sim['questions'] == j + 1].head(N)  # limit to N\n",
    "    for i, response in question_responses.iterrows():\n",
    "        n = i % N\n",
    "        kt_vote = 1 - kendalltau(response['votes'], response['options'])[0]\n",
    "        kt_prediction = 1 - kendalltau(response['predictions'], response['options'])[0]\n",
    "        kendall_tau_votes[n, j] = kt_vote\n",
    "        kendall_tau_predictions[n, j] = kt_prediction\n",
    "\n",
    "# Build Stan data dict\n",
    "stan_data_synth = {\n",
    "    'N': N,\n",
    "    'K': K,\n",
    "    'J': J,\n",
    "    'kendall_tau_votes': kendall_tau_votes,\n",
    "    'kendall_tau_predictions': kendall_tau_predictions,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2ef9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_synth = run_model_with_priors(stan_data_synth, prior_sets['case1'])\n",
    "results['synth_case1'] = fit_synth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a325900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mean_params(fit):\n",
    "    summary = fit.summary()\n",
    "    param_names = summary['summary_rownames']\n",
    "    means = summary['summary'][:, 0]\n",
    "    return dict(zip(param_names, means))\n",
    "\n",
    "# Extract means\n",
    "original_params = extract_mean_params(results['case1'])\n",
    "synth_params = extract_mean_params(results['synth_case1'])\n",
    "\n",
    "# Collect results\n",
    "comparison_records = []\n",
    "\n",
    "param_keys = [\n",
    "    'dispersion_vote_expert',\n",
    "    'dispersion_vote_intermediate',\n",
    "    'dispersion_vote_nonexpert',\n",
    "    'dispersion_pred_expert',\n",
    "    'dispersion_pred_intermediate',\n",
    "    'dispersion_pred_nonexpert',\n",
    "    'prob_group[1]',  # Expert\n",
    "    'prob_group[2]',  # Intermediate\n",
    "    'prob_group[3]',  # Non-expert\n",
    "]\n",
    "\n",
    "for key in param_keys:\n",
    "    rel_error = abs(original_params[key] - synth_params[key]) / original_params[key]\n",
    "    comparison_records.append({\n",
    "        'parameter': key,\n",
    "        'original': original_params[key],\n",
    "        'synthetic': synth_params[key],\n",
    "        'relative_error': rel_error\n",
    "    })\n",
    "    print(f\"{key}: Relative Error = {rel_error:.4f}\")\n",
    "\n",
    "# Save to CSV\n",
    "df_compare = pd.DataFrame(comparison_records)\n",
    "df_compare.to_csv('group3_parameter_recovery_comparison_mallows.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stan_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
